{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.9.1-cp313-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-2.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "#nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing news as suggested\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus = brown.sents(categories='news')\n",
    "news_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus = news_corpus[:300] # taking small subset for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "news_flatten = flatten(news_corpus)\n",
    "len(news_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(news_flatten))\n",
    "vocab.append('<UNK>')\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {w:i for (i, w) in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {v:k for k, v in word2index.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, corpus, window_size=2):\n",
    "    # Make skip gram of custom size window\n",
    "    skip_grams = []\n",
    "\n",
    "    for sent in corpus:\n",
    "        for target_index in range(window_size, len(sent) - window_size):\n",
    "            target = word2index[sent[target_index]]\n",
    "            context = []\n",
    "            count = window_size # count of context words to pick on the left and right\n",
    "            while count > 0:\n",
    "                # for default window, it will get the left most and right most word\n",
    "                # then the second left most and second right most word\n",
    "                context.append(word2index[sent[target_index - count]])\n",
    "                context.append(word2index[sent[target_index + count]])\n",
    "                count -= 1\n",
    "\n",
    "            for word in context:\n",
    "                skip_grams.append([target, word])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the method\n",
    "batch_size = 2 # mini-batch size\n",
    "input_batch, target_batch = random_batch(batch_size, news_corpus)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "\n",
    "        self.embedding_center   = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_outside  = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "    def forward(self, center, outside, all_vocab):\n",
    "        center_embedding    = self.embedding_center(center)     # as seen in above example, size: (batch_size, 1, embedding_size)\n",
    "        outside_embedding   = self.embedding_outside(outside)   # (batch_size, 1, embedding_size)\n",
    "        all_vocab_embedding = self.embedding_outside(all_vocab) # (batch_size, vocab_size, embedding_size)\n",
    "\n",
    "        numerator   = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        # (b_size, 1, emb_size) @ (b_size, emb_size, 1) = (b_size, 1, 1) -> (b_size, 1)\n",
    "\n",
    "\n",
    "        denominator = all_vocab_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        # (b_size, vocab_size, emb_size) @ (b_size, emb_size, 1) = (b_size, vocab_size, 1) -> (b_size, vocab_size)\n",
    "\n",
    "        denominator_sum = torch.sum(torch.exp(denominator), 1)\n",
    "\n",
    "        loss = -torch.mean(torch.log(numerator / denominator_sum)) # scalar\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 2 # mini-batch size\n",
    "embedding_size = 2\n",
    "model          = Skipgram(vocab_size, embedding_size)\n",
    "model_skipgram = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(w):\n",
    "    if word2index.get(w) is not None:\n",
    "        return word2index[w]\n",
    "    else:\n",
    "        return word2index['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "#use for the normalized term in the probability calculation\n",
    "all_vocabs = prepare_sequence(list(vocab), word2index).expand(batch_size, len(vocab))  # [batch_size, voc_size]\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch = random_batch(batch_size, news_corpus)\n",
    "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "\n",
    "    # changing to cuda\n",
    "    input_batch  = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    all_vocabs   = all_vocabs.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model_skipgram(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model_skipgram.state_dict(), 'model/skipgram_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model_skipgram, open('model/skipgram.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numericalization\n",
    "id = word2index[word]\n",
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tensor = torch.LongTensor([id])\n",
    "id_tensor = id_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the embedding by averaging\n",
    "v_embed = model_skipgram.embedding_center(id_tensor)\n",
    "u_embed = model_skipgram.embedding_outside(id_tensor)\n",
    "\n",
    "v_embed, u_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average to get the word embedding\n",
    "word_embed = (v_embed + u_embed) / 2\n",
    "word_embed[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_skip_gram(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    id_tensor = id_tensor.to(device)\n",
    "    v_embed = model_skipgram.embedding_center(id_tensor)\n",
    "    u_embed = model_skipgram.embedding_outside(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "government = get_embed_skip_gram('government')\n",
    "officials = get_embed_skip_gram('officials')\n",
    "administration = get_embed_skip_gram('administration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"government vs officials: {cos_sim(government, officials):.4f}\")\n",
    "print(f\"government vs administration: {cos_sim(government, administration):.4f}\")\n",
    "print(f\"government vs government: {cos_sim(government, government):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram (Negative Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(news_flatten)\n",
    "num_total_words = sum([c for w, c in word_count.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for vo in vocab:\n",
    "    uw = word_count[vo] / num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / Z)\n",
    "    unigram_table.extend([vo] * uw_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):  #(1, k)\n",
    "        target_index = targets[i].item()\n",
    "        nsample      = []\n",
    "        while (len(nsample) < k):\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "        \n",
    "    return torch.cat(neg_samples) #batch_size, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "x, y = random_batch(batch_size, news_corpus)\n",
    "x_tensor = torch.LongTensor(x)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "x_tensor = x_tensor.to(device)\n",
    "y_tensor = y_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "neg_samples = negative_sampling(y_tensor, unigram_table, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed   = self.embedding_center(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.embedding_outside(negative) #(bs, k, emb_size)\n",
    "\n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1)\n",
    "        \n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2\n",
    "voc_size = len(vocab)\n",
    "model = SkipgramNeg(voc_size, emb_size)\n",
    "model_neg = model.to(device)\n",
    "neg_samples = neg_samples.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model_neg(x_tensor, y_tensor, neg_samples)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_neg.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, news_corpus)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "\n",
    "    #move to cuda\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    label_tensor = label_tensor.to(device)\n",
    "    \n",
    "    #predict\n",
    "    neg_samples = negative_sampling(label_tensor, unigram_table, k)\n",
    "    neg_samples = neg_samples.to(device)\n",
    "\n",
    "    loss = model_neg(input_tensor, label_tensor, neg_samples)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss:2.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model_neg.state_dict(), 'model/skipgram_neg_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_neg, open('model/skipgram_neg.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_neg_sample(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    id_tensor = id_tensor.to(device)\n",
    "    v_embed = model_neg.embedding_center(id_tensor)\n",
    "    u_embed = model_neg.embedding_outside(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "government = get_embed_neg_sample('government')\n",
    "officials = get_embed_neg_sample('officials')\n",
    "administration = get_embed_neg_sample('administration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"government vs officials: {cos_sim(government, officials):.4f}\")\n",
    "print(f\"government vs administration: {cos_sim(government, administration):.4f}\")\n",
    "print(f\"government vs government: {cos_sim(government, government):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipgram(window_size = 2):\n",
    "    # Make skip gram of custom size window\n",
    "    skip_grams = []\n",
    "\n",
    "    for sent in news_corpus:\n",
    "        for target_index in range(window_size, len(sent) - window_size):\n",
    "            target = sent[target_index]\n",
    "            context = []\n",
    "            count = window_size # count of context words to pick on the left and right\n",
    "            while count > 0:\n",
    "                # for default window, it will get the left most and right most word\n",
    "                # then the second left most and second right most word\n",
    "                context.append(sent[target_index - count])\n",
    "                context.append(sent[target_index + count])\n",
    "                count -= 1\n",
    "\n",
    "            for word in context:\n",
    "                skip_grams.append((target, word))\n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = get_skipgram(2)\n",
    "skip_grams[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik_skipgram = Counter(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply a normalized function...don't worry too much\n",
    "def weighting(w_i, w_j, X_ik):\n",
    "        \n",
    "    #check whether the co-occurrences exist between these two words\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1  #if does not exist, set it to 1\n",
    "                \n",
    "    x_max = 100 #100 # fixed in paper  #cannot exceed 100 counts\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if co-occurrence does not exceed 100, scale it based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max)**alpha  #scale it\n",
    "    else:\n",
    "        result = 1  #if is greater than max, set it to 1 maximum\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {}  #for keeping the co-occurences\n",
    "weighting_dic = {} #scaling the percentage of sampling\n",
    "\n",
    "for bigram in combinations_with_replacement(vocab, 2):\n",
    "    if X_ik_skipgram.get(bigram) is not None:  #matches \n",
    "        co_occer = X_ik_skipgram[bigram]  #get the count from what we already counted\n",
    "        X_ik[bigram] = co_occer + 1 # + 1 for stability issue\n",
    "        X_ik[(bigram[1],bigram[0])] = co_occer+1   #count also for the opposite\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)\n",
    "\n",
    "# print(f\"{X_ik=}\")\n",
    "# print(f\"{weighting_dic=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #convert to id since our skip_grams is word, not yet id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_coocs  = []\n",
    "    random_weightings = []\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams_id[i][1]])  # context word, e.g., 3\n",
    "        \n",
    "        #get cooc\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "                    \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the method\n",
    "batch_size = 2 # mini-batch size\n",
    "input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, news_corpus, skip_grams, X_ik, weighting_dic)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)\n",
    "print(\"Cooc: \", cooc_batch)\n",
    "print(\"Weighting: \", weighting_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_center = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_outside = nn.Embedding(vocab_size, embed_size) # out embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_center(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_outside(target_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        \n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs already got log\n",
    "        loss = weighting*torch.pow(inner_product +center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 10 # mini-batch size\n",
    "embedding_size = 2 #so we can later plot\n",
    "model_glove    = GloVe(voc_size, embedding_size)\n",
    "model_glove    = model_glove.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_glove.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, news_corpus, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch  = torch.LongTensor(input_batch)         #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch)        #[batch_size, 1]\n",
    "    cooc_batch   = torch.FloatTensor(cooc_batch)         #[batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch) #[batch_size, 1]\n",
    "\n",
    "    # to cuda\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    cooc_batch = cooc_batch.to(device)\n",
    "    weighting_batch = weighting_batch.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model_glove(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model_glove.state_dict(), 'model/glove_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model using pickle\n",
    "pickle.dump(model_glove, open('model/glove.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_glove(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    id_tensor = id_tensor.to(device)\n",
    "    v_embed = model_glove.embedding_center(id_tensor)\n",
    "    u_embed = model_glove.embedding_outside(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "government = get_embed_glove('government')\n",
    "officials = get_embed_glove('officials')\n",
    "administration = get_embed_glove('administration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"government vs officials: {cos_sim(government, officials):.4f}\")\n",
    "print(f\"government vs administration: {cos_sim(government, administration):.4f}\")\n",
    "print(f\"government vs government: {cos_sim(government, government):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove (Genism)\n",
    "Source credit: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = datapath('glove.6B.100d.txt')\n",
    "model_genism = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Word similarity\n",
    "similarity = model_genism.similarity('king', 'queen')\n",
    "print(f\"Similarity between 'king' and 'queen': {similarity:.4f}\")\n",
    "\n",
    "# Example: Word analogy\n",
    "result = model_genism.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "print(\"King - Man + Woman = \", result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(word1, word2, word3, embeddings, word_to_index, index_to_word):\n",
    "    # Get vectors for w1, w2, w3\n",
    "    vec1 = np.array(embeddings(word1))\n",
    "    vec2 = np.array(embeddings(word2))\n",
    "    vec3 = np.array(embeddings(word3))\n",
    "\n",
    "    # Calculate the predicted vector\n",
    "    predicted_vec = vec1 - vec2 + vec3\n",
    "\n",
    "    # Find the closest word by cosine similarity\n",
    "    max_similarity = -1\n",
    "    best_word = None\n",
    "    for word, index in word_to_index.items():\n",
    "        if word in [word1, word2, word3]:  # Skip the input words\n",
    "            continue\n",
    "        similarity = cos_sim(predicted_vec, embeddings(word))\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_word = word\n",
    "\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "def evaluate_analogies(analogy_dataset, embeddings, word_to_index):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for analogy in analogy_dataset:\n",
    "        word1, word2, word3, word4 = analogy\n",
    "        if word1 not in word_to_index or word2 not in word_to_index or word3 not in word_to_index or word4 not in word_to_index:\n",
    "            continue  # Skip if any word is not in the vocabulary\n",
    "        predicted_word = predict_word(word1, word2, word3, embeddings, word_to_index, {v: k for k, v in word_to_index.items()})\n",
    "        if predicted_word == word4:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Source credit: https://www.fit.vut.cz/person/imikolov/public/rnnlm/word-test.v1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"capital-common-countries.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "semantic_dataset = []\n",
    "for line in lines:\n",
    "    # Split the line into words\n",
    "    words = line.strip().split()\n",
    "    if len(words) == 4:\n",
    "        semantic_dataset.append([words[0], words[1], words[2], words[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"past-tense.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "past_tense_dataset = []\n",
    "for line in lines:\n",
    "    words = line.strip().split()\n",
    "    if len(words) == 4:\n",
    "        past_tense_dataset.append([words[0], words[1], words[2], words[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check\n",
    "semantic_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_tense_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_analogies(past_tense_dataset, get_embed_skip_gram, word2index)\n",
    "print(f\"Syntactic Accuracy - skipgram: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_analogies(past_tense_dataset, get_embed_neg_sample, word2index)\n",
    "print(f\"Syntactic Accuracy - negative sample: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_analogies(past_tense_dataset, get_embed_glove, word2index)\n",
    "print(f\"Syntactic Accuracy - glove: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model_genism.evaluate_word_analogies(\"past_tense_lines.txt\")[0]\n",
    "print(f\"Syntactic Accuracy - gensim: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_analogies(semantic_dataset, get_embed_skip_gram, word2index)\n",
    "print(f\"Semantic Accuracy - skipgram: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_analogies(semantic_dataset, get_embed_neg_sample, word2index)\n",
    "print(f\"Semantic Accuracy - negative sample: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_analogies(semantic_dataset, get_embed_glove, word2index)\n",
    "print(f\"Semantic Accuracy - glove: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model_genism.evaluate_word_analogies(\"capital.txt\")[0]\n",
    "print(f\"Semantic Accuracy - gensim: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Window Size | Training Loss | Training Time (sec) | Syntactic Accuracy (%) | Semantic Accuracy (%) |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| Skipgram    | 2     | 7.60     | 310     | 0.0     | 0.0   |\n",
    "| Skipgram (Neg)   | 2     | 1.89     | 171     | 0.0     | 0.0     |\n",
    "| Glove    | 2     | 2.33    | 34     | 0.0    | 0.0     |\n",
    "| Glove (Genism)    | -     | -     | -     | 55.45     | 93.87     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Test\n",
    "Source credit: http://alfonseca.org/eng/research/wordsim353.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['Word 1', 'Word 2', 'Similarity Index']\n",
    "\n",
    "df = pd.read_csv('wordsim_relatedness_goldstandard.txt', sep='\\t', header=None, names=columns)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    word_1 = row['Word 1']\n",
    "    word_2 = row['Word 2']\n",
    "\n",
    "    try:\n",
    "        embed_1_neg_samp    = get_embed_neg_sample(word_1)\n",
    "        embed_2_neg_samp    = get_embed_neg_sample(word_2)\n",
    "        embed_1_skip_gram   = get_embed_skip_gram(word_1)\n",
    "        embed_2_skip_gram   = get_embed_skip_gram(word_2)\n",
    "        embed_1_glove       = get_embed_glove(word_1)\n",
    "        embed_2_glove       = get_embed_glove(word_2)\n",
    "\n",
    "    except KeyError:\n",
    "        # Replacing missing embeddings with the embedding of '<UNK>'\n",
    "        embed_1_neg_samp    = get_embed_neg_sample('<UNK>')\n",
    "        embed_2_neg_samp    = get_embed_neg_sample('<UNK>')\n",
    "        embed_1_skip_gram   = get_embed_skip_gram('<UNK>')\n",
    "        embed_2_skip_gram   = get_embed_skip_gram('<UNK>')\n",
    "        embed_1_glove       = get_embed_glove('<UNK>')\n",
    "        embed_2_glove       = get_embed_glove('<UNK>')\n",
    "\n",
    "    # Computing dot product\n",
    "    df.at[index, 'dot_product_neg_samp'] = np.dot(embed_1_neg_samp, embed_2_neg_samp)\n",
    "    df.at[index, 'dot_product_skip_gram'] = np.dot(embed_1_skip_gram, embed_2_skip_gram)\n",
    "    df.at[index, 'dot_product_glove'] = np.dot(embed_1_glove, embed_2_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Computing the Spearman correlation\n",
    "correlation_pos, _ = spearmanr(df['Similarity Index'], df['dot_product_skip_gram'])\n",
    "correlation_neg, _ = spearmanr(df['Similarity Index'], df['dot_product_neg_samp'])\n",
    "correlation_glove, _ = spearmanr(df['Similarity Index'], df['dot_product_glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Spearman Correlation Coefficient of Skipgram: {correlation_pos:.4f}\")\n",
    "print(f\"Spearman Correlation Coefficient of Skipgram with Negative Sampling: {correlation_neg:.4f}\")\n",
    "print(f\"Spearman Correlation Coefficient of Glove: {correlation_glove:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding y_true based on the mean of similarity index in the df\n",
    "y_true = df['Similarity Index'].mean()\n",
    "\n",
    "print(f\"y_true: {y_true:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_coefficient = model_genism.evaluate_word_pairs('wordsim_relatedness_goldstandard.txt')\n",
    "print(f\"Spearman Correlation Correlation coefficient of Glove (genism): {correlation_coefficient[1][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Skipgram | NEG | GloVe | GloVe (genism) | Y_true |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| MSE    | 0.011     | 0.0385     | -0.03     | 0.5     | 5.29   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_for_corpus(model, words):\n",
    "    embeddings = {}\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            index = word2index[word]\n",
    "        except KeyError:\n",
    "            index = word2index['<UNK>']\n",
    "\n",
    "        word_tensor = torch.LongTensor([index])\n",
    "        word_tensor = word_tensor.to(device)\n",
    "\n",
    "        embed_c = model.embedding_center(word_tensor)\n",
    "        embed_o = model.embedding_outside(word_tensor)\n",
    "        embed = (embed_c + embed_o) / 2\n",
    "\n",
    "        # return as dictionary with key as the word and value as the array of its embedding\n",
    "        embeddings[word] = np.array([embed[0][0].item(), embed[0][1].item()])\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_whole_glove = get_embed_for_corpus(model_glove, vocab)\n",
    "embed_whole_neg_skg = get_embed_for_corpus(model_neg, vocab)\n",
    "embed_whole_skg = get_embed_for_corpus(model_glove, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/model_gensim.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model_genism, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/embed_skipgram_negative.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(embed_whole_neg_skg, pickle_file)\n",
    "\n",
    "print(f\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/embed_skipgram.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(embed_whole_skg, pickle_file)\n",
    "\n",
    "print(f\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/embed_glove.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(embed_whole_glove, pickle_file)\n",
    "\n",
    "print(f\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, for window size of 2 - skipgram had the highest (7.60) training loss while Negative skipgram had the lowest (1.89). Glove also performed much better compared to skipgram with loss of 2.33.\n",
    "\n",
    "In case of Training time each model were trained for 5000 epoch. Skipgram took the longest time with 310s and Glove took the least amount of time with 34s. Negative sampling took 171s. \n",
    "\n",
    "All three models coded from scratch performed bad compared to Genism. This is because of the small corpus size and window size. All 3 models (Skipgram, Skipgram with negative sampling, Glove) had syantactic and semantic accuracy of 0%. This was expected because of the limitations of our corpus. Glove (Genism) on the other achieved Syntactic accuracy of 55.45% \n",
    "and Semantic Accuracy of 93.87%.\n",
    "\n",
    "Furthermore, for Spearman Correlation Coefficient - Genism outperforms other models with correlation score of `0.5`. The other 3 models showed poor correlation which suggests that predicted rankings do not closely match with ground truth. So our embeddings has poor correlation with human judgement.\n",
    "\n",
    "In conclusion, given the small corpus size, window size and embedding dimension - our 'made from scratch' models performed poorly. Given better hyperparameter tunings like embeddig dimensions, learning rate and even number of epochs, the models can be refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
