{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.13/site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/lib/python3.13/site-packages (from gensim) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/lib/python3.13/site-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /opt/anaconda3/lib/python3.13/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.13/site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/daki/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/daki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/daki/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to /Users/daki/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from nltk.corpus import reuters\n",
    "from collections import Counter\n",
    "nltk.download('reuters')\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"reuters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acq',\n",
       " 'alum',\n",
       " 'barley',\n",
       " 'bop',\n",
       " 'carcass',\n",
       " 'castor-oil',\n",
       " 'cocoa',\n",
       " 'coconut',\n",
       " 'coconut-oil',\n",
       " 'coffee',\n",
       " 'copper',\n",
       " 'copra-cake',\n",
       " 'corn',\n",
       " 'cotton',\n",
       " 'cotton-oil',\n",
       " 'cpi',\n",
       " 'cpu',\n",
       " 'crude',\n",
       " 'dfl',\n",
       " 'dlr',\n",
       " 'dmk',\n",
       " 'earn',\n",
       " 'fuel',\n",
       " 'gas',\n",
       " 'gnp',\n",
       " 'gold',\n",
       " 'grain',\n",
       " 'groundnut',\n",
       " 'groundnut-oil',\n",
       " 'heat',\n",
       " 'hog',\n",
       " 'housing',\n",
       " 'income',\n",
       " 'instal-debt',\n",
       " 'interest',\n",
       " 'ipi',\n",
       " 'iron-steel',\n",
       " 'jet',\n",
       " 'jobs',\n",
       " 'l-cattle',\n",
       " 'lead',\n",
       " 'lei',\n",
       " 'lin-oil',\n",
       " 'livestock',\n",
       " 'lumber',\n",
       " 'meal-feed',\n",
       " 'money-fx',\n",
       " 'money-supply',\n",
       " 'naphtha',\n",
       " 'nat-gas',\n",
       " 'nickel',\n",
       " 'nkr',\n",
       " 'nzdlr',\n",
       " 'oat',\n",
       " 'oilseed',\n",
       " 'orange',\n",
       " 'palladium',\n",
       " 'palm-oil',\n",
       " 'palmkernel',\n",
       " 'pet-chem',\n",
       " 'platinum',\n",
       " 'potato',\n",
       " 'propane',\n",
       " 'rand',\n",
       " 'rape-oil',\n",
       " 'rapeseed',\n",
       " 'reserves',\n",
       " 'retail',\n",
       " 'rice',\n",
       " 'rubber',\n",
       " 'rye',\n",
       " 'ship',\n",
       " 'silver',\n",
       " 'sorghum',\n",
       " 'soy-meal',\n",
       " 'soy-oil',\n",
       " 'soybean',\n",
       " 'strategic-metal',\n",
       " 'sugar',\n",
       " 'sun-meal',\n",
       " 'sun-oil',\n",
       " 'sunseed',\n",
       " 'tea',\n",
       " 'tin',\n",
       " 'trade',\n",
       " 'veg-oil',\n",
       " 'wheat',\n",
       " 'wpi',\n",
       " 'yen',\n",
       " 'zinc']"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choosing news as suggested\n",
    "reuters.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['COCOA', 'EXPORTERS', 'EXPECTED', 'TO', 'LIMIT', 'SALES', 'Major', 'cocoa', 'exporters', 'are', 'likely', 'to', 'limit', 'sales', 'in', 'the', 'weeks', 'ahead', 'in', 'an', 'effort', 'to', 'boost', 'world', 'prices', ',', 'sources', 'close', 'to', 'a', 'meeting', 'of', 'the', 'Cocoa', 'Producers', 'Alliance', '(', 'CPA', ')', 'said', '.'], ['The', 'sources', 'said', 'the', 'depressed', 'world', 'market', 'had', 'been', 'one', 'of', 'the', 'main', 'topics', 'discussed', 'in', 'a', 'closed', 'door', 'meeting', 'of', 'the', '11', '-', 'member', 'CPA', 'which', 'began', 'on', 'Monday', '.'], ...]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_corpus = reuters.sents(categories='cocoa')\n",
    "news_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "print(sorted(reuters.categories()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus = news_corpus[:300] # taking small subset for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8699"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "news_flatten = flatten(news_corpus)\n",
    "len(news_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['early', 'provide', 'making', 'did', '77']"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(news_flatten))\n",
    "vocab.append('<UNK>')\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {w:i for (i, w) in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {v:k for k, v in word2index.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1889"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, corpus, window_size=2):\n",
    "    # Make skip gram of custom size window\n",
    "    skip_grams = []\n",
    "\n",
    "    for sent in corpus:\n",
    "        for target_index in range(window_size, len(sent) - window_size):\n",
    "            target = word2index[sent[target_index]]\n",
    "            context = []\n",
    "            count = window_size # count of context words to pick on the left and right\n",
    "            while count > 0:\n",
    "                # for default window, it will get the left most and right most word\n",
    "                # then the second left most and second right most word\n",
    "                context.append(word2index[sent[target_index - count]])\n",
    "                context.append(word2index[sent[target_index + count]])\n",
    "                count -= 1\n",
    "\n",
    "            for word in context:\n",
    "                skip_grams.append([target, word])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[1171]\n",
      " [1111]]\n",
      "Target:  [[ 667]\n",
      " [1243]]\n"
     ]
    }
   ],
   "source": [
    "#testing the method\n",
    "batch_size = 2 # mini-batch size\n",
    "input_batch, target_batch = random_batch(batch_size, news_corpus)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "\n",
    "        self.embedding_center   = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_outside  = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "    def forward(self, center, outside, all_vocab):\n",
    "        center_embedding    = self.embedding_center(center)     # as seen in above example, size: (batch_size, 1, embedding_size)\n",
    "        outside_embedding   = self.embedding_outside(outside)   # (batch_size, 1, embedding_size)\n",
    "        all_vocab_embedding = self.embedding_outside(all_vocab) # (batch_size, vocab_size, embedding_size)\n",
    "\n",
    "        numerator   = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        # (b_size, 1, emb_size) @ (b_size, emb_size, 1) = (b_size, 1, 1) -> (b_size, 1)\n",
    "\n",
    "\n",
    "        denominator = all_vocab_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        # (b_size, vocab_size, emb_size) @ (b_size, emb_size, 1) = (b_size, vocab_size, 1) -> (b_size, vocab_size)\n",
    "\n",
    "        denominator_sum = torch.sum(torch.exp(denominator), 1)\n",
    "\n",
    "        loss = -torch.mean(torch.log(numerator / denominator_sum)) # scalar\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 2 # mini-batch size\n",
    "embedding_size = 2\n",
    "model          = Skipgram(vocab_size, embedding_size)\n",
    "model_skipgram = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(w):\n",
    "    if word2index.get(w) is not None:\n",
    "        return word2index[w]\n",
    "    else:\n",
    "        return word2index['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1889])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "#use for the normalized term in the probability calculation\n",
    "all_vocabs = prepare_sequence(list(vocab), word2index).expand(batch_size, len(vocab))  # [batch_size, voc_size]\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 8.070057 | time: 0m 0s\n",
      "Epoch: 2000 | cost: 7.823939 | time: 0m 0s\n",
      "Epoch: 3000 | cost: 7.000368 | time: 0m 0s\n",
      "Epoch: 4000 | cost: 7.269746 | time: 0m 0s\n",
      "Epoch: 5000 | cost: 7.897872 | time: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch = random_batch(batch_size, news_corpus)\n",
    "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "\n",
    "    # changing to cuda\n",
    "    input_batch  = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    all_vocabs   = all_vocabs.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model_skipgram(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model_skipgram.state_dict(), 'model/skipgram_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model_skipgram, open('model/skipgram.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numericalization\n",
    "id = word2index[word]\n",
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tensor = torch.LongTensor([id])\n",
    "id_tensor = id_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0020, -0.6032]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[ 0.0830, -0.3254]], grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the embedding by averaging\n",
    "v_embed = model_skipgram.embedding_center(id_tensor)\n",
    "u_embed = model_skipgram.embedding_outside(id_tensor)\n",
    "\n",
    "v_embed, u_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.4643, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average to get the word embedding\n",
    "word_embed = (v_embed + u_embed) / 2\n",
    "word_embed[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_skip_gram(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    id_tensor = id_tensor.to(device)\n",
    "    v_embed = model_skipgram.embedding_center(id_tensor)\n",
    "    u_embed = model_skipgram.embedding_outside(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "government = get_embed_skip_gram('formal')\n",
    "officials = get_embed_skip_gram('almost')\n",
    "administration = get_embed_skip_gram('Malaysian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formal vs almost: 0.1466\n",
      "formal vs Malaysian: 0.5393\n",
      "formal vs formal: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"formal vs almost: {cos_sim(formal, almost):.4f}\")\n",
    "print(f\"formal vs Malaysian: {cos_sim(formal, Malaysian):.4f}\")\n",
    "print(f\"formal vs formal: {cos_sim(formal, formal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram (Negative Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(news_flatten)\n",
    "num_total_words = sum([c for w, c in word_count.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8699"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for vo in vocab:\n",
    "    uw = word_count[vo] / num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / Z)\n",
    "    unigram_table.extend([vo] * uw_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({',': 100,\n",
       "         'the': 98,\n",
       "         '.': 90,\n",
       "         'to': 64,\n",
       "         'of': 53,\n",
       "         'and': 48,\n",
       "         'in': 43,\n",
       "         'said': 41,\n",
       "         'cocoa': 38,\n",
       "         'a': 34,\n",
       "         'stock': 30,\n",
       "         'The': 30,\n",
       "         'buffer': 29,\n",
       "         'for': 27,\n",
       "         'is': 26,\n",
       "         'be': 22,\n",
       "         'from': 22,\n",
       "         'ICCO': 22,\n",
       "         'on': 21,\n",
       "         '-': 20,\n",
       "         'market': 19,\n",
       "         'are': 19,\n",
       "         '\"': 19,\n",
       "         'prices': 18,\n",
       "         'at': 18,\n",
       "         'by': 18,\n",
       "         'as': 17,\n",
       "         'will': 17,\n",
       "         '000': 17,\n",
       "         'manager': 16,\n",
       "         'with': 16,\n",
       "         'tonnes': 16,\n",
       "         \"'\": 16,\n",
       "         'that': 16,\n",
       "         'which': 15,\n",
       "         'Cocoa': 15,\n",
       "         'not': 15,\n",
       "         'International': 14,\n",
       "         '1': 14,\n",
       "         'COCOA': 14,\n",
       "         'it': 14,\n",
       "         '/': 14,\n",
       "         'have': 14,\n",
       "         'rules': 14,\n",
       "         'they': 14,\n",
       "         'were': 13,\n",
       "         'delegates': 13,\n",
       "         's': 13,\n",
       "         '(': 13,\n",
       "         'was': 13,\n",
       "         'would': 12,\n",
       "         'price': 12,\n",
       "         'pct': 11,\n",
       "         ')': 11,\n",
       "         'council': 11,\n",
       "         'producers': 11,\n",
       "         'has': 11,\n",
       "         'been': 10,\n",
       "         'buy': 10,\n",
       "         'he': 10,\n",
       "         'commodity': 10,\n",
       "         'agreement': 10,\n",
       "         'its': 10,\n",
       "         'world': 10,\n",
       "         'sales': 10,\n",
       "         'an': 10,\n",
       "         'countries': 9,\n",
       "         'than': 9,\n",
       "         '2': 9,\n",
       "         'two': 9,\n",
       "         'new': 9,\n",
       "         'if': 9,\n",
       "         'could': 9,\n",
       "         'Organization': 9,\n",
       "         'mln': 9,\n",
       "         'dlrs': 9,\n",
       "         '1986': 9,\n",
       "         'London': 8,\n",
       "         'spokesman': 8,\n",
       "         'traders': 8,\n",
       "         'tonne': 8,\n",
       "         'or': 8,\n",
       "         'stg': 8,\n",
       "         ',\"': 8,\n",
       "         'crop': 8,\n",
       "         'West': 8,\n",
       "         'up': 8,\n",
       "         'some': 8,\n",
       "         'buying': 8,\n",
       "         'one': 8,\n",
       "         'into': 8,\n",
       "         'sources': 8,\n",
       "         'meeting': 8,\n",
       "         'U': 8,\n",
       "         'more': 8,\n",
       "         'S': 8,\n",
       "         'differentials': 7,\n",
       "         'limit': 7,\n",
       "         'pact': 7,\n",
       "         'consumers': 7,\n",
       "         'this': 7,\n",
       "         'June': 7,\n",
       "         'do': 7,\n",
       "         'members': 7,\n",
       "         'member': 7,\n",
       "         'but': 7,\n",
       "         'chairman': 7,\n",
       "         '4': 7,\n",
       "         'BUFFER': 7,\n",
       "         'year': 7,\n",
       "         'can': 7,\n",
       "         'producer': 7,\n",
       "         'first': 7,\n",
       "         'agreements': 6,\n",
       "         'had': 6,\n",
       "         'Malaysian': 6,\n",
       "         '0': 6,\n",
       "         'week': 6,\n",
       "         'New': 6,\n",
       "         'TO': 6,\n",
       "         'Malaysia': 6,\n",
       "         '3': 6,\n",
       "         'total': 6,\n",
       "         'previous': 6,\n",
       "         'under': 6,\n",
       "         'compromise': 6,\n",
       "         'after': 6,\n",
       "         'official': 6,\n",
       "         'only': 6,\n",
       "         'expected': 6,\n",
       "         'also': 6,\n",
       "         '5': 6,\n",
       "         'purchases': 6,\n",
       "         'beans': 6,\n",
       "         'consumer': 6,\n",
       "         'trade': 6,\n",
       "         'pacts': 6,\n",
       "         'weeks': 6,\n",
       "         '6': 5,\n",
       "         'Bra': 5,\n",
       "         'offers': 5,\n",
       "         'York': 5,\n",
       "         'high': 5,\n",
       "         'different': 5,\n",
       "         'per': 5,\n",
       "         'operations': 5,\n",
       "         'when': 5,\n",
       "         'A': 5,\n",
       "         'Brazil': 5,\n",
       "         'told': 5,\n",
       "         'STOCK': 5,\n",
       "         'added': 5,\n",
       "         'non': 5,\n",
       "         'July': 5,\n",
       "         '15': 5,\n",
       "         'case': 5,\n",
       "         'Ghana': 5,\n",
       "         'there': 5,\n",
       "         '87': 5,\n",
       "         'today': 5,\n",
       "         'officials': 5,\n",
       "         'likely': 5,\n",
       "         'It': 5,\n",
       "         'last': 5,\n",
       "         'bags': 5,\n",
       "         'his': 5,\n",
       "         'long': 5,\n",
       "         'Friday': 5,\n",
       "         'support': 5,\n",
       "         'three': 5,\n",
       "         'any': 5,\n",
       "         'limited': 5,\n",
       "         'about': 5,\n",
       "         'Kanon': 5,\n",
       "         'They': 5,\n",
       "         'exporters': 4,\n",
       "         'day': 4,\n",
       "         'still': 4,\n",
       "         'over': 4,\n",
       "         '25': 4,\n",
       "         'before': 4,\n",
       "         'Agreement': 4,\n",
       "         'Indonesia': 4,\n",
       "         '.\"': 4,\n",
       "         'Trade': 4,\n",
       "         'made': 4,\n",
       "         'quality': 4,\n",
       "         'executive': 4,\n",
       "         'May': 4,\n",
       "         'Dec': 4,\n",
       "         'bought': 4,\n",
       "         'against': 4,\n",
       "         'within': 4,\n",
       "         'so': 4,\n",
       "         'work': 4,\n",
       "         'because': 4,\n",
       "         'reached': 4,\n",
       "         'Officials': 4,\n",
       "         'due': 4,\n",
       "         'recently': 4,\n",
       "         'CSCE': 4,\n",
       "         'export': 4,\n",
       "         'January': 4,\n",
       "         'We': 4,\n",
       "         'times': 4,\n",
       "         '600': 4,\n",
       "         'all': 4,\n",
       "         'their': 4,\n",
       "         'European': 4,\n",
       "         'Reuters': 4,\n",
       "         'selling': 4,\n",
       "         'same': 4,\n",
       "         '10': 4,\n",
       "         'CPA': 4,\n",
       "         'If': 4,\n",
       "         'around': 4,\n",
       "         'according': 4,\n",
       "         'Coffee': 4,\n",
       "         'country': 4,\n",
       "         'effect': 4,\n",
       "         'nearby': 4,\n",
       "         'both': 4,\n",
       "         'level': 4,\n",
       "         'earlier': 4,\n",
       "         'economic': 4,\n",
       "         'quarter': 4,\n",
       "         'low': 4,\n",
       "         'sugar': 4,\n",
       "         'Bahia': 4,\n",
       "         'April': 4,\n",
       "         'coffee': 4,\n",
       "         'international': 4,\n",
       "         '9': 4,\n",
       "         'cut': 4,\n",
       "         'main': 4,\n",
       "         'each': 4,\n",
       "         'German': 4,\n",
       "         'output': 4,\n",
       "         'period': 4,\n",
       "         'months': 4,\n",
       "         'set': 4,\n",
       "         '1987': 4,\n",
       "         'dry': 4,\n",
       "         'part': 4,\n",
       "         'no': 4,\n",
       "         'Ivory': 4,\n",
       "         'Coast': 4,\n",
       "         'should': 4,\n",
       "         'temporao': 4,\n",
       "         'take': 4,\n",
       "         'early': 3,\n",
       "         'did': 3,\n",
       "         'full': 3,\n",
       "         'agreed': 3,\n",
       "         'industry': 3,\n",
       "         'second': 3,\n",
       "         'March': 3,\n",
       "         '),': 3,\n",
       "         'between': 3,\n",
       "         'way': 3,\n",
       "         'board': 3,\n",
       "         'origin': 3,\n",
       "         'reality': 3,\n",
       "         'cocoas': 3,\n",
       "         'described': 3,\n",
       "         'good': 3,\n",
       "         'Denis': 3,\n",
       "         'Japan': 3,\n",
       "         'W': 3,\n",
       "         'farmers': 3,\n",
       "         'negotiations': 3,\n",
       "         'Dutch': 3,\n",
       "         'taken': 3,\n",
       "         'developing': 3,\n",
       "         'shipment': 3,\n",
       "         'Sept': 3,\n",
       "         'ICCH': 3,\n",
       "         'rose': 3,\n",
       "         'vice': 3,\n",
       "         'how': 3,\n",
       "         'weather': 3,\n",
       "         'Ministry': 3,\n",
       "         'means': 3,\n",
       "         'much': 3,\n",
       "         'him': 3,\n",
       "         'accord': 3,\n",
       "         'processors': 3,\n",
       "         'sold': 3,\n",
       "         'meet': 3,\n",
       "         'reflect': 3,\n",
       "         'now': 3,\n",
       "         'cent': 3,\n",
       "         'next': 3,\n",
       "         'close': 3,\n",
       "         'wheat': 3,\n",
       "         'Aug': 3,\n",
       "         'end': 3,\n",
       "         'producing': 3,\n",
       "         'tin': 3,\n",
       "         'free': 3,\n",
       "         'far': 3,\n",
       "         'conditions': 3,\n",
       "         'September': 3,\n",
       "         'African': 3,\n",
       "         'term': 3,\n",
       "         'harvest': 3,\n",
       "         'such': 3,\n",
       "         'origins': 3,\n",
       "         'Foreign': 3,\n",
       "         'may': 3,\n",
       "         'estimated': 3,\n",
       "         'Minister': 3,\n",
       "         'begin': 3,\n",
       "         'immediately': 3,\n",
       "         'noted': 3,\n",
       "         'most': 3,\n",
       "         'Smith': 3,\n",
       "         'Exchange': 3,\n",
       "         'below': 3,\n",
       "         'contracts': 3,\n",
       "         'very': 3,\n",
       "         'trends': 3,\n",
       "         'positions': 3,\n",
       "         'what': 3,\n",
       "         'RULES': 3,\n",
       "         'EC': 3,\n",
       "         'Grace': 3,\n",
       "         'February': 3,\n",
       "         'previously': 3,\n",
       "         'since': 3,\n",
       "         'down': 3,\n",
       "         'fall': 3,\n",
       "         'remain': 3,\n",
       "         'Baron': 3,\n",
       "         'normal': 3,\n",
       "         'other': 3,\n",
       "         'expand': 3,\n",
       "         'Berisford': 3,\n",
       "         'production': 3,\n",
       "         'maximum': 3,\n",
       "         'Council': 3,\n",
       "         'He': 3,\n",
       "         'limits': 3,\n",
       "         'well': 3,\n",
       "         'agree': 3,\n",
       "         'Brazilian': 3,\n",
       "         'force': 3,\n",
       "         'say': 3,\n",
       "         'see': 3,\n",
       "         'month': 3,\n",
       "         'come': 3,\n",
       "         'Ivorian': 3,\n",
       "         'sterling': 3,\n",
       "         'drought': 3,\n",
       "         'accepted': 3,\n",
       "         'lower': 3,\n",
       "         'need': 3,\n",
       "         'range': 3,\n",
       "         'But': 3,\n",
       "         'fixed': 3,\n",
       "         'Comissaria': 3,\n",
       "         'PNG': 3,\n",
       "         'unlikely': 3,\n",
       "         'deposit': 3,\n",
       "         'major': 3,\n",
       "         'forward': 3,\n",
       "         'delegate': 3,\n",
       "         '1985': 3,\n",
       "         'daily': 3,\n",
       "         '27': 3,\n",
       "         'continue': 3,\n",
       "         'pounds': 3,\n",
       "         'In': 3,\n",
       "         'Agriculture': 3,\n",
       "         'make': 3,\n",
       "         'exporting': 3,\n",
       "         'Consumers': 3,\n",
       "         'coming': 3,\n",
       "         'currently': 3,\n",
       "         'Special': 2,\n",
       "         'another': 2,\n",
       "         'although': 2,\n",
       "         'Delegates': 2,\n",
       "         'unhappy': 2,\n",
       "         'Kuala': 2,\n",
       "         'based': 2,\n",
       "         'Oct': 2,\n",
       "         'statement': 2,\n",
       "         'growing': 2,\n",
       "         'offer': 2,\n",
       "         'COUNCIL': 2,\n",
       "         'through': 2,\n",
       "         'back': 2,\n",
       "         'policy': 2,\n",
       "         'levels': 2,\n",
       "         'possible': 2,\n",
       "         'Drawing': 2,\n",
       "         '250': 2,\n",
       "         'intermediate': 2,\n",
       "         'standard': 2,\n",
       "         'off': 2,\n",
       "         'surpluses': 2,\n",
       "         'Indonesian': 2,\n",
       "         '20': 2,\n",
       "         'present': 2,\n",
       "         'participation': 2,\n",
       "         'trading': 2,\n",
       "         'Fraternite': 2,\n",
       "         'effort': 2,\n",
       "         'GMT': 2,\n",
       "         'five': 2,\n",
       "         'boost': 2,\n",
       "         'Asia': 2,\n",
       "         'France': 2,\n",
       "         'trader': 2,\n",
       "         '55': 2,\n",
       "         'clauses': 2,\n",
       "         'weight': 2,\n",
       "         'consuming': 2,\n",
       "         'until': 2,\n",
       "         'Wednesday': 2,\n",
       "         'funds': 2,\n",
       "         'considered': 2,\n",
       "         'taking': 2,\n",
       "         'basis': 2,\n",
       "         'chocolate': 2,\n",
       "         '100': 2,\n",
       "         'Matin': 2,\n",
       "         'Kouame': 2,\n",
       "         'ranges': 2,\n",
       "         'Buffer': 2,\n",
       "         '13': 2,\n",
       "         'rubber': 2,\n",
       "         'Department': 2,\n",
       "         'restricted': 2,\n",
       "         'stabilise': 2,\n",
       "         'Monday': 2,\n",
       "         '190': 2,\n",
       "         'out': 2,\n",
       "         'pod': 2,\n",
       "         'problem': 2,\n",
       "         'bean': 2,\n",
       "         'Commission': 2,\n",
       "         'system': 2,\n",
       "         'discuss': 2,\n",
       "         'real': 2,\n",
       "         'rather': 2,\n",
       "         'Producers': 2,\n",
       "         '40': 2,\n",
       "         'meant': 2,\n",
       "         '50': 2,\n",
       "         'sessions': 2,\n",
       "         '22': 2,\n",
       "         'buyers': 2,\n",
       "         'Community': 2,\n",
       "         'stabilisation': 2,\n",
       "         '300': 2,\n",
       "         'surplus': 2,\n",
       "         'issue': 2,\n",
       "         'fairly': 2,\n",
       "         'There': 2,\n",
       "         'committee': 2,\n",
       "         'regions': 2,\n",
       "         'Consumer': 2,\n",
       "         'time': 2,\n",
       "         'little': 2,\n",
       "         'Peter': 2,\n",
       "         'president': 2,\n",
       "         'working': 2,\n",
       "         'fund': 2,\n",
       "         'spot': 2,\n",
       "         'current': 2,\n",
       "         'hand': 2,\n",
       "         'COFFEE': 2,\n",
       "         'ports': 2,\n",
       "         'those': 2,\n",
       "         'October': 2,\n",
       "         'Association': 2,\n",
       "         'area': 2,\n",
       "         'satisfied': 2,\n",
       "         'Rights': 2,\n",
       "         'cost': 2,\n",
       "         'INRA': 2,\n",
       "         'days': 2,\n",
       "         'problems': 2,\n",
       "         'capital': 2,\n",
       "         'expanding': 2,\n",
       "         'cents': 2,\n",
       "         '400': 2,\n",
       "         'open': 2,\n",
       "         'Industry': 2,\n",
       "         'approach': 2,\n",
       "         'required': 2,\n",
       "         'wanted': 2,\n",
       "         '21': 2,\n",
       "         'IMMEDIATELY': 2,\n",
       "         'large': 2,\n",
       "         '137': 2,\n",
       "         'just': 2,\n",
       "         '00': 2,\n",
       "         'plan': 2,\n",
       "         'final': 2,\n",
       "         'them': 2,\n",
       "         'affected': 2,\n",
       "         'intermittent': 2,\n",
       "         'largest': 2,\n",
       "         'criteria': 2,\n",
       "         't': 2,\n",
       "         'This': 2,\n",
       "         'source': 2,\n",
       "         'These': 2,\n",
       "         'Lumpur': 2,\n",
       "         'scheme': 2,\n",
       "         'intervention': 2,\n",
       "         'successful': 2,\n",
       "         'further': 2,\n",
       "         'move': 2,\n",
       "         'quota': 2,\n",
       "         '60': 2,\n",
       "         'rise': 2,\n",
       "         'Stock': 2,\n",
       "         'exports': 2,\n",
       "         'sell': 2,\n",
       "         'seems': 2,\n",
       "         'action': 2,\n",
       "         'aid': 2,\n",
       "         'ITC': 2,\n",
       "         'during': 2,\n",
       "         'state': 2,\n",
       "         '150': 2,\n",
       "         'talks': 2,\n",
       "         'points': 2,\n",
       "         'COMPROMISE': 2,\n",
       "         'withholding': 2,\n",
       "         'package': 2,\n",
       "         'elected': 2,\n",
       "         'provide': 1,\n",
       "         'making': 1,\n",
       "         '77': 1,\n",
       "         'supply': 1,\n",
       "         'I': 1,\n",
       "         'direct': 1,\n",
       "         'Kobena': 1,\n",
       "         'lack': 1,\n",
       "         'group': 1,\n",
       "         'differential': 1,\n",
       "         'CACEX': 1,\n",
       "         'thought': 1,\n",
       "         '590': 1,\n",
       "         'london': 1,\n",
       "         'season': 1,\n",
       "         'BUY': 1,\n",
       "         'basic': 1,\n",
       "         'Togo': 1,\n",
       "         'post': 1,\n",
       "         'fulfil': 1,\n",
       "         'appears': 1,\n",
       "         'running': 1,\n",
       "         'disguise': 1,\n",
       "         'Kusumaatmadja': 1,\n",
       "         'rot': 1,\n",
       "         'Carey': 1,\n",
       "         'EXPECTED': 1,\n",
       "         'delivered': 1,\n",
       "         'soon': 1,\n",
       "         '1230': 1,\n",
       "         'Asian': 1,\n",
       "         'ends': 1,\n",
       "         'results': 1,\n",
       "         'effective': 1,\n",
       "         'light': 1,\n",
       "         'resulted': 1,\n",
       "         'formal': 1,\n",
       "         'weekly': 1,\n",
       "         'topics': 1,\n",
       "         'HELP': 1,\n",
       "         'Board': 1,\n",
       "         'sliding': 1,\n",
       "         'banks': 1,\n",
       "         'shunned': 1,\n",
       "         'Buyers': 1,\n",
       "         'analyses': 1,\n",
       "         'keep': 1,\n",
       "         'DAILY': 1,\n",
       "         'directly': 1,\n",
       "         'Salvador': 1,\n",
       "         'midway': 1,\n",
       "         'Executive': 1,\n",
       "         'insist': 1,\n",
       "         'reconvene': 1,\n",
       "         'GERMAN': 1,\n",
       "         'FOR': 1,\n",
       "         'business': 1,\n",
       "         'reduction': 1,\n",
       "         'ENDS': 1,\n",
       "         'Mohammed': 1,\n",
       "         'almost': 1,\n",
       "         'zero': 1,\n",
       "         'BAHIA': 1,\n",
       "         'ESTIMATE': 1,\n",
       "         'swiftly': 1,\n",
       "         'transaction': 1,\n",
       "         'downward': 1,\n",
       "         '213': 1,\n",
       "         'Unlike': 1,\n",
       "         'responsive': 1,\n",
       "         'mean': 1,\n",
       "         'we': 1,\n",
       "         'nations': 1,\n",
       "         '33': 1,\n",
       "         'governments': 1,\n",
       "         'Manager': 1,\n",
       "         'weights': 1,\n",
       "         'relevance': 1,\n",
       "         'mechanism': 1,\n",
       "         'serving': 1,\n",
       "         'Non': 1,\n",
       "         'always': 1,\n",
       "         'company': 1,\n",
       "         'Reuter': 1,\n",
       "         'account': 1,\n",
       "         'links': 1,\n",
       "         'Country': 1,\n",
       "         '67': 1,\n",
       "         'water': 1,\n",
       "         'interesting': 1,\n",
       "         'agreeing': 1,\n",
       "         'guidelines': 1,\n",
       "         'mainstays': 1,\n",
       "         'focus': 1,\n",
       "         'procedures': 1,\n",
       "         'included': 1,\n",
       "         'party': 1,\n",
       "         'harvesting': 1,\n",
       "         'dictate': 1,\n",
       "         'Juergen': 1,\n",
       "         'regulatory': 1,\n",
       "         'feel': 1,\n",
       "         'combine': 1,\n",
       "         'harsher': 1,\n",
       "         'incidences': 1,\n",
       "         'avoid': 1,\n",
       "         'prefer': 1,\n",
       "         'used': 1,\n",
       "         'traditional': 1,\n",
       "         'contentious': 1,\n",
       "         'type': 1,\n",
       "         'United': 1,\n",
       "         'hundred': 1,\n",
       "         'reported': 1,\n",
       "         'getting': 1,\n",
       "         'assigned': 1,\n",
       "         'record': 1,\n",
       "         'Uruguay': 1,\n",
       "         'consignment': 1,\n",
       "         'partly': 1,\n",
       "         'chastened': 1,\n",
       "         'FIRST': 1,\n",
       "         'lows': 1,\n",
       "         'view': 1,\n",
       "         '68': 1,\n",
       "         'permitted': 1,\n",
       "         'palm': 1,\n",
       "         'POSITIVE': 1,\n",
       "         'SALES': 1,\n",
       "         'starts': 1,\n",
       "         'Development': 1,\n",
       "         'PLC': 1,\n",
       "         'rejecting': 1,\n",
       "         'participate': 1,\n",
       "         'accords': 1,\n",
       "         '81': 1,\n",
       "         '130': 1,\n",
       "         'PRICE': 1,\n",
       "         '1400': 1,\n",
       "         'presented': 1,\n",
       "         'REUTER': 1,\n",
       "         'growers': 1,\n",
       "         '88': 1,\n",
       "         'arroba': 1,\n",
       "         'units': 1,\n",
       "         '--': 1,\n",
       "         'odour': 1,\n",
       "         'appeals': 1,\n",
       "         'analysts': 1,\n",
       "         'intend': 1,\n",
       "         'No': 1,\n",
       "         'arrangements': 1,\n",
       "         'adjustment': 1,\n",
       "         'emphasis': 1,\n",
       "         'cuts': 1,\n",
       "         '221': 1,\n",
       "         'prospects': 1,\n",
       "         'honesty': 1,\n",
       "         'Shearson': 1,\n",
       "         'think': 1,\n",
       "         'include': 1,\n",
       "         'government': 1,\n",
       "         'revise': 1,\n",
       "         'ahead': 1,\n",
       "         'weren': 1,\n",
       "         'yet': 1,\n",
       "         'Nicaraguan': 1,\n",
       "         'follows': 1,\n",
       "         'shall': 1,\n",
       "         'places': 1,\n",
       "         'Lorenzo': 1,\n",
       "         'recent': 1,\n",
       "         'political': 1,\n",
       "         'qualities': 1,\n",
       "         'rainfall': 1,\n",
       "         'benefitted': 1,\n",
       "         'later': 1,\n",
       "         'alleviating': 1,\n",
       "         'oriented': 1,\n",
       "         'backing': 1,\n",
       "         'fiddling': 1,\n",
       "         '80': 1,\n",
       "         'arrival': 1,\n",
       "         'concession': 1,\n",
       "         'important': 1,\n",
       "         'Carryover': 1,\n",
       "         'interest': 1,\n",
       "         'December': 1,\n",
       "         'inside': 1,\n",
       "         'Importing': 1,\n",
       "         'together': 1,\n",
       "         'greater': 1,\n",
       "         'never': 1,\n",
       "         'offering': 1,\n",
       "         'middlemen': 1,\n",
       "         'prove': 1,\n",
       "         'Prices': 1,\n",
       "         'strict': 1,\n",
       "         'registered': 1,\n",
       "         'material': 1,\n",
       "         'GRA': 1,\n",
       "         'WEST': 1,\n",
       "         'process': 1,\n",
       "         '380': 1,\n",
       "         '880': 1,\n",
       "         'discussion': 1,\n",
       "         'manufacturer': 1,\n",
       "         'Covertible': 1,\n",
       "         'Paris': 1,\n",
       "         'figures': 1,\n",
       "         'systems': 1,\n",
       "         'CROPS': 1,\n",
       "         'past': 1,\n",
       "         'unsuitable': 1,\n",
       "         'distribution': 1,\n",
       "         'Industries': 1,\n",
       "         'appear': 1,\n",
       "         'example': 1,\n",
       "         'C': 1,\n",
       "         'least': 1,\n",
       "         'gaining': 1,\n",
       "         'Final': 1,\n",
       "         'expect': 1,\n",
       "         'completely': 1,\n",
       "         'Evans': 1,\n",
       "         'Destinations': 1,\n",
       "         'Pact': 1,\n",
       "         'normally': 1,\n",
       "         'identified': 1,\n",
       "         'controlled': 1,\n",
       "         'citing': 1,\n",
       "         'mature': 1,\n",
       "         'difficulty': 1,\n",
       "         'link': 1,\n",
       "         'COMMODITY': 1,\n",
       "         'TAKE': 1,\n",
       "         'DELEGATES': 1,\n",
       "         'Western': 1,\n",
       "         'pull': 1,\n",
       "         'Legal': 1,\n",
       "         'questions': 1,\n",
       "         '375': 1,\n",
       "         'WELCOMES': 1,\n",
       "         'exceed': 1,\n",
       "         'occurred': 1,\n",
       "         'comments': 1,\n",
       "         'Lehman': 1,\n",
       "         '271': 1,\n",
       "         'Netherlands': 1,\n",
       "         'moved': 1,\n",
       "         'shipping': 1,\n",
       "         'broke': 1,\n",
       "         'lb': 1,\n",
       "         '11': 1,\n",
       "         'rates': 1,\n",
       "         'reafforestation': 1,\n",
       "         'sides': 1,\n",
       "         'manufacturers': 1,\n",
       "         'reach': 1,\n",
       "         'precise': 1,\n",
       "         'Barros': 1,\n",
       "         'states': 1,\n",
       "         'you': 1,\n",
       "         'published': 1,\n",
       "         'ACTION': 1,\n",
       "         'exemplified': 1,\n",
       "         'held': 1,\n",
       "         'However': 1,\n",
       "         'COMMISSIONER': 1,\n",
       "         'Bros': 1,\n",
       "         'division': 1,\n",
       "         'fatty': 1,\n",
       "         'Precipitation': 1,\n",
       "         'manoeuvre': 1,\n",
       "         'act': 1,\n",
       "         'difficult': 1,\n",
       "         '337': 1,\n",
       "         'wet': 1,\n",
       "         'expensive': 1,\n",
       "         'ratify': 1,\n",
       "         'itself': 1,\n",
       "         'priced': 1,\n",
       "         'estates': 1,\n",
       "         'specified': 1,\n",
       "         'linked': 1,\n",
       "         'everyone': 1,\n",
       "         'varying': 1,\n",
       "         '..': 1,\n",
       "         'too': 1,\n",
       "         'late': 1,\n",
       "         'eliminated': 1,\n",
       "         'deposits': 1,\n",
       "         'uses': 1,\n",
       "         'ACCEPTANCE': 1,\n",
       "         'tool': 1,\n",
       "         'own': 1,\n",
       "         'obtaining': 1,\n",
       "         'tried': 1,\n",
       "         'Siew': 1,\n",
       "         'dismiss': 1,\n",
       "         'Did': 1,\n",
       "         'help': 1,\n",
       "         'exported': 1,\n",
       "         'whose': 1,\n",
       "         'Graham': 1,\n",
       "         'emerged': 1,\n",
       "         'many': 1,\n",
       "         'BSM': 1,\n",
       "         'tripled': 1,\n",
       "         'however': 1,\n",
       "         'widespread': 1,\n",
       "         'several': 1,\n",
       "         'AFTER': 1,\n",
       "         '750': 1,\n",
       "         '75': 1,\n",
       "         'going': 1,\n",
       "         'SEEN': 1,\n",
       "         'significant': 1,\n",
       "         'NOT': 1,\n",
       "         'chances': 1,\n",
       "         'Argentina': 1,\n",
       "         'reduced': 1,\n",
       "         'Growers': 1,\n",
       "         'doubts': 1,\n",
       "         'winners': 1,\n",
       "         'TOWARDS': 1,\n",
       "         'contribute': 1,\n",
       "         'successive': 1,\n",
       "         'cruzados': 1,\n",
       "         'SUGAR': 1,\n",
       "         'believe': 1,\n",
       "         'rains': 1,\n",
       "         'door': 1,\n",
       "         'objectives': 1,\n",
       "         'JUNE': 1,\n",
       "         'closing': 1,\n",
       "         'Alejandro': 1,\n",
       "         '26': 1,\n",
       "         'contributions': 1,\n",
       "         'calendar': 1,\n",
       "         'less': 1,\n",
       "         'EFFECTIVE': 1,\n",
       "         'Doubts': 1,\n",
       "         'Demico': 1,\n",
       "         'Rubber': 1,\n",
       "         'operational': 1,\n",
       "         'Clearing': 1,\n",
       "         'EXCHANGE': 1,\n",
       "         'noting': 1,\n",
       "         'regulating': 1,\n",
       "         'commodities': 1,\n",
       "         'Bank': 1,\n",
       "         'provides': 1,\n",
       "         'doldrums': 1,\n",
       "         'began': 1,\n",
       "         'direction': 1,\n",
       "         'currency': 1,\n",
       "         'RESTRUCTURES': 1,\n",
       "         'absent': 1,\n",
       "         'consultations': 1,\n",
       "         'relatively': 1,\n",
       "         'ACCORD': 1,\n",
       "         'edge': 1,\n",
       "         'GRINDINGS': 1,\n",
       "         'LIMIT': 1,\n",
       "         'ago': 1,\n",
       "         'sanctions': 1,\n",
       "         'step': 1,\n",
       "         'communication': 1,\n",
       "         'impact': 1,\n",
       "         'tomorrow': 1,\n",
       "         'initiatives': 1,\n",
       "         'participants': 1,\n",
       "         'biggest': 1,\n",
       "         'stringent': 1,\n",
       "         'stick': 1,\n",
       "         'receipt': 1,\n",
       "         'individual': 1,\n",
       "         'ended': 1,\n",
       "         '655': 1,\n",
       "         'hours': 1,\n",
       "         'continuing': 1,\n",
       "         'Differentials': 1,\n",
       "         'survive': 1,\n",
       "         'settling': 1,\n",
       "         'kilos': 1,\n",
       "         'key': 1,\n",
       "         'prohibited': 1,\n",
       "         'committees': 1,\n",
       "         'fluctuations': 1,\n",
       "         'Differential': 1,\n",
       "         'TEMPORAO': 1,\n",
       "         'Papua': 1,\n",
       "         'ORIENTED': 1,\n",
       "         '.,': 1,\n",
       "         'followed': 1,\n",
       "         'falling': 1,\n",
       "         'FOB': 1,\n",
       "         'DETAILED': 1,\n",
       "         'borrowing': 1,\n",
       "         'similar': 1,\n",
       "         'Banco': 1,\n",
       "         'half': 1,\n",
       "         'us': 1,\n",
       "         'director': 1,\n",
       "         'RATIFY': 1,\n",
       "         'MARKET': 1,\n",
       "         'essential': 1,\n",
       "         'where': 1,\n",
       "         'cif': 1,\n",
       "         'TRADERS': 1,\n",
       "         '\")': 1,\n",
       "         'Gill': 1,\n",
       "         '16': 1,\n",
       "         'reports': 1,\n",
       "         'question': 1,\n",
       "         'treasurer': 1,\n",
       "         'various': 1,\n",
       "         'higher': 1,\n",
       "         'instrument': 1,\n",
       "         'British': 1,\n",
       "         '200': 1,\n",
       "         'Producing': 1,\n",
       "         'cfa': 1,\n",
       "         '935': 1,\n",
       "         'enough': 1,\n",
       "         'hrs': 1,\n",
       "         'indicator': 1,\n",
       "         'joint': 1,\n",
       "         'announce': 1,\n",
       "         'doesn': 1,\n",
       "         'undermining': 1,\n",
       "         'rule': 1,\n",
       "         'stocks': 1,\n",
       "         'Kee': 1,\n",
       "         'obligations': 1,\n",
       "         'does': 1,\n",
       "         'Negotiators': 1,\n",
       "         'predicted': 1,\n",
       "         'acid': 1,\n",
       "         'compares': 1,\n",
       "         'So': 1,\n",
       "         'result': 1,\n",
       "         'bsm': 1,\n",
       "         '1500': 1,\n",
       "         'among': 1,\n",
       "         'Affairs': 1,\n",
       "         'want': 1,\n",
       "         'discount': 1,\n",
       "         'Under': 1,\n",
       "         '284': 1,\n",
       "         'Direct': 1,\n",
       "         'bring': 1,\n",
       "         'argued': 1,\n",
       "         'unable': 1,\n",
       "         'potential': 1,\n",
       "         'inferior': 1,\n",
       "         'pre': 1,\n",
       "         'amount': 1,\n",
       "         'Common': 1,\n",
       "         '1100': 1,\n",
       "         '36': 1,\n",
       "         'cumulative': 1,\n",
       "         'reconvenes': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):  #(1, k)\n",
    "        target_index = targets[i].item()\n",
    "        nsample      = []\n",
    "        while (len(nsample) < k):\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "        \n",
    "    return torch.cat(neg_samples) #batch_size, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "x, y = random_batch(batch_size, news_corpus)\n",
    "x_tensor = torch.LongTensor(x)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "x_tensor = x_tensor.to(device)\n",
    "y_tensor = y_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "neg_samples = negative_sampling(y_tensor, unigram_table, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([591])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1279,  343, 1803, 1803,  387])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed   = self.embedding_center(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.embedding_outside(negative) #(bs, k, emb_size)\n",
    "\n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1)\n",
    "        \n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2\n",
    "voc_size = len(vocab)\n",
    "model = SkipgramNeg(voc_size, emb_size)\n",
    "model_neg = model.to(device)\n",
    "neg_samples = neg_samples.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7375, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model_neg(x_tensor, y_tensor, neg_samples)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_neg.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1000 | Loss: 2.562646\n",
      "Epoch   2000 | Loss: 1.240909\n",
      "Epoch   3000 | Loss: 1.524035\n",
      "Epoch   4000 | Loss: 0.893156\n",
      "Epoch   5000 | Loss: 1.732999\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, news_corpus)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "\n",
    "    #move to cuda\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    label_tensor = label_tensor.to(device)\n",
    "    \n",
    "    #predict\n",
    "    neg_samples = negative_sampling(label_tensor, unigram_table, k)\n",
    "    neg_samples = neg_samples.to(device)\n",
    "\n",
    "    loss = model_neg(input_tensor, label_tensor, neg_samples)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss:2.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model_neg.state_dict(), 'model/skipgram_neg_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_neg, open('model/skipgram_neg.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_neg_sample(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    id_tensor = id_tensor.to(device)\n",
    "    v_embed = model_neg.embedding_center(id_tensor)\n",
    "    u_embed = model_neg.embedding_outside(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "government = get_embed_neg_sample('formal')\n",
    "officials = get_embed_neg_sample('almost')\n",
    "administration = get_embed_neg_sample('Malaysian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formal vs almost: 0.4797\n",
      "formal vs Malaysian: 0.8579\n",
      "formal vs formal: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"formal vs almost: {cos_sim(government, officials):.4f}\")\n",
    "print(f\"formal vs Malaysian: {cos_sim(government, administration):.4f}\")\n",
    "print(f\"formal vs formal: {cos_sim(government, government):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipgram(window_size = 2):\n",
    "    # Make skip gram of custom size window\n",
    "    skip_grams = []\n",
    "\n",
    "    for sent in news_corpus:\n",
    "        for target_index in range(window_size, len(sent) - window_size):\n",
    "            target = sent[target_index]\n",
    "            context = []\n",
    "            count = window_size # count of context words to pick on the left and right\n",
    "            while count > 0:\n",
    "                # for default window, it will get the left most and right most word\n",
    "                # then the second left most and second right most word\n",
    "                context.append(sent[target_index - count])\n",
    "                context.append(sent[target_index + count])\n",
    "                count -= 1\n",
    "\n",
    "            for word in context:\n",
    "                skip_grams.append((target, word))\n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EXPECTED', 'COCOA'),\n",
       " ('EXPECTED', 'LIMIT'),\n",
       " ('EXPECTED', 'EXPORTERS'),\n",
       " ('EXPECTED', 'TO'),\n",
       " ('TO', 'EXPORTERS'),\n",
       " ('TO', 'SALES'),\n",
       " ('TO', 'EXPECTED'),\n",
       " ('TO', 'LIMIT')]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_grams = get_skipgram(2)\n",
    "skip_grams[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik_skipgram = Counter(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply a normalized function...don't worry too much\n",
    "def weighting(w_i, w_j, X_ik):\n",
    "        \n",
    "    #check whether the co-occurrences exist between these two words\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1  #if does not exist, set it to 1\n",
    "                \n",
    "    x_max = 100 #100 # fixed in paper  #cannot exceed 100 counts\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if co-occurrence does not exceed 100, scale it based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max)**alpha  #scale it\n",
    "    else:\n",
    "        result = 1  #if is greater than max, set it to 1 maximum\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {}  #for keeping the co-occurences\n",
    "weighting_dic = {} #scaling the percentage of sampling\n",
    "\n",
    "for bigram in combinations_with_replacement(vocab, 2):\n",
    "    if X_ik_skipgram.get(bigram) is not None:  #matches \n",
    "        co_occer = X_ik_skipgram[bigram]  #get the count from what we already counted\n",
    "        X_ik[bigram] = co_occer + 1 # + 1 for stability issue\n",
    "        X_ik[(bigram[1],bigram[0])] = co_occer+1   #count also for the opposite\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)\n",
    "\n",
    "# print(f\"{X_ik=}\")\n",
    "# print(f\"{weighting_dic=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #convert to id since our skip_grams is word, not yet id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_coocs  = []\n",
    "    random_weightings = []\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams_id[i][1]])  # context word, e.g., 3\n",
    "        \n",
    "        #get cooc\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "                    \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[ 173]\n",
      " [1111]]\n",
      "Target:  [[1135]\n",
      " [ 383]]\n",
      "Cooc:  [[1.38629436]\n",
      " [2.94443898]]\n",
      "Weighting:  [[0.08944272]\n",
      " [0.28778304]]\n"
     ]
    }
   ],
   "source": [
    "#testing the method\n",
    "batch_size = 2 # mini-batch size\n",
    "input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, news_corpus, skip_grams, X_ik, weighting_dic)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)\n",
    "print(\"Cooc: \", cooc_batch)\n",
    "print(\"Weighting: \", weighting_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_center = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_outside = nn.Embedding(vocab_size, embed_size) # out embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_center(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_outside(target_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        \n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs already got log\n",
    "        loss = weighting*torch.pow(inner_product +center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 10 # mini-batch size\n",
    "embedding_size = 2 #so we can later plot\n",
    "model_glove    = GloVe(voc_size, embedding_size)\n",
    "model_glove    = model_glove.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_glove.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 4.172056 | time: 0m 0s\n",
      "Epoch: 2000 | cost: 3.891329 | time: 0m 0s\n",
      "Epoch: 3000 | cost: 9.241829 | time: 0m 0s\n",
      "Epoch: 4000 | cost: 1.085584 | time: 0m 0s\n",
      "Epoch: 5000 | cost: 4.912088 | time: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, news_corpus, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch  = torch.LongTensor(input_batch)         #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch)        #[batch_size, 1]\n",
    "    cooc_batch   = torch.FloatTensor(cooc_batch)         #[batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch) #[batch_size, 1]\n",
    "\n",
    "    # to cuda\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    cooc_batch = cooc_batch.to(device)\n",
    "    weighting_batch = weighting_batch.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model_glove(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model_glove.state_dict(), 'model/glove_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model using pickle\n",
    "pickle.dump(model_glove, open('model/glove.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_glove(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    id_tensor = id_tensor.to(device)\n",
    "    v_embed = model_glove.embedding_center(id_tensor)\n",
    "    u_embed = model_glove.embedding_outside(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "government = get_embed_glove('formal')\n",
    "officials = get_embed_glove('almost')\n",
    "administration = get_embed_glove('Malaysian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formal vs almost: -0.0456\n",
      "formal vs Malaysian: 0.4791\n",
      "formal vs formal: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"formal vs almost: {cos_sim(government, officials):.4f}\")\n",
    "print(f\"formal vs Malaysian: {cos_sim(government, administration):.4f}\")\n",
    "print(f\"formal vs formal: {cos_sim(government, government):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove (Genism)\n",
    "Source credit: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = datapath('glove.6B.100d.txt')\n",
    "model_genism = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'queen': 0.7508\n",
      "King - Man + Woman =  queen\n"
     ]
    }
   ],
   "source": [
    "# Example: Word similarity\n",
    "similarity = model_genism.similarity('king', 'queen')\n",
    "print(f\"Similarity between 'king' and 'queen': {similarity:.4f}\")\n",
    "\n",
    "# Example: Word analogy\n",
    "result = model_genism.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "print(\"King - Man + Woman = \", result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(word1, word2, word3, embeddings, word_to_index, index_to_word):\n",
    "    # Get vectors for w1, w2, w3\n",
    "    vec1 = np.array(embeddings(word1))\n",
    "    vec2 = np.array(embeddings(word2))\n",
    "    vec3 = np.array(embeddings(word3))\n",
    "\n",
    "    # Calculate the predicted vector\n",
    "    predicted_vec = vec1 - vec2 + vec3\n",
    "\n",
    "    # Find the closest word by cosine similarity\n",
    "    max_similarity = -1\n",
    "    best_word = None\n",
    "    for word, index in word_to_index.items():\n",
    "        if word in [word1, word2, word3]:  # Skip the input words\n",
    "            continue\n",
    "        similarity = cos_sim(predicted_vec, embeddings(word))\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_word = word\n",
    "\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "def evaluate_analogies(analogy_dataset, embeddings, word_to_index):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for analogy in analogy_dataset:\n",
    "        word1, word2, word3, word4 = analogy\n",
    "        if word1 not in word_to_index or word2 not in word_to_index or word3 not in word_to_index or word4 not in word_to_index:\n",
    "            continue  # Skip if any word is not in the vocabulary\n",
    "        predicted_word = predict_word(word1, word2, word3, embeddings, word_to_index, {v: k for k, v in word_to_index.items()})\n",
    "        if predicted_word == word4:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Source credit: https://www.fit.vut.cz/person/imikolov/public/rnnlm/word-test.v1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"custom_city_country_analogies.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "semantic_dataset = []\n",
    "for line in lines:\n",
    "    # Split the line into words\n",
    "    words = line.strip().split()\n",
    "    if len(words) == 4:\n",
    "        semantic_dataset.append([words[0], words[1], words[2], words[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"custom_verb_past_analogies.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "past_tense_dataset = []\n",
    "for line in lines:\n",
    "    words = line.strip().split()\n",
    "    if len(words) == 4:\n",
    "        past_tense_dataset.append([words[0], words[1], words[2], words[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Sydney', 'Australia', 'Melbourne', 'Australia'],\n",
       " ['Manchester', 'United_Kingdom', 'Liverpool', 'United_Kingdom'],\n",
       " ['Barcelona', 'Spain', 'Valencia', 'Spain'],\n",
       " ['Munich', 'Germany', 'Hamburg', 'Germany'],\n",
       " ['Milan', 'Italy', 'Naples', 'Italy']]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick check\n",
    "semantic_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['build', 'built', 'grow', 'grew'],\n",
       " ['teach', 'taught', 'catch', 'caught'],\n",
       " ['send', 'sent', 'lend', 'lent'],\n",
       " ['spend', 'spent', 'bend', 'bent'],\n",
       " ['choose', 'chose', 'freeze', 'froze']]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_tense_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy - skipgram: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(past_tense_dataset, get_embed_skip_gram, word2index)\n",
    "print(f\"Syntactic Accuracy - skipgram: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy - negative sample: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(past_tense_dataset, get_embed_neg_sample, word2index)\n",
    "print(f\"Syntactic Accuracy - negative sample: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy - glove: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(past_tense_dataset, get_embed_glove, word2index)\n",
    "print(f\"Syntactic Accuracy - glove: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy - gensim: 55.45%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model_genism.evaluate_word_analogies(\"past_tense_lines.txt\")[0]\n",
    "print(f\"Syntactic Accuracy - gensim: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy - skipgram: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(semantic_dataset, get_embed_skip_gram, word2index)\n",
    "print(f\"Semantic Accuracy - skipgram: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy - negative sample: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(semantic_dataset, get_embed_neg_sample, word2index)\n",
    "print(f\"Semantic Accuracy - negative sample: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy - glove: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(semantic_dataset, get_embed_glove, word2index)\n",
    "print(f\"Semantic Accuracy - glove: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing section header before line #0 in capital.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[392]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m accuracy = \u001b[43mmodel_genism\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_word_analogies\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapital.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSemantic Accuracy - gensim: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/gensim/models/keyedvectors.py:1348\u001b[39m, in \u001b[36mKeyedVectors.evaluate_word_analogies\u001b[39m\u001b[34m(self, analogies, restrict_vocab, case_insensitive, dummy4unknown, similarity_function)\u001b[39m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m section:\n\u001b[32m-> \u001b[39m\u001b[32m1348\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMissing section header before line #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (line_no, analogies))\n\u001b[32m   1349\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1350\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m case_insensitive:\n",
      "\u001b[31mValueError\u001b[39m: Missing section header before line #0 in capital.txt"
     ]
    }
   ],
   "source": [
    "accuracy = model_genism.evaluate_word_analogies(\"capital.txt\")[0]\n",
    "print(f\"Semantic Accuracy - gensim: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Window Size | Training Loss | Training Time (sec) | Syntactic Accuracy (%) | Semantic Accuracy (%) |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| Skipgram    | 2     | 7.60     | 310     | 0.0     | 0.0   |\n",
    "| Skipgram (Neg)   | 2     | 1.89     | 171     | 0.0     | 0.0     |\n",
    "| Glove    | 2     | 2.33    | 34     | 0.0    | 0.0     |\n",
    "| Glove (Genism)    | -     | -     | -     | 55.45     | 93.87     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Test\n",
    "Source credit: http://alfonseca.org/eng/research/wordsim353.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Similarity Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>teacher instructor 9.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doctor physician 9.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lawyer attorney 9.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>student pupil 8.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>city metropolis 8.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>sad unhappy 9.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>angry furious 8.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>calm peaceful 8.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>strong powerful 9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>weak fragile 8.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Word 1  Word 2  Similarity Index\n",
       "0   teacher instructor 9.3     NaN               NaN\n",
       "1     doctor physician 9.4     NaN               NaN\n",
       "2      lawyer attorney 9.1     NaN               NaN\n",
       "3        student pupil 8.8     NaN               NaN\n",
       "4      city metropolis 8.7     NaN               NaN\n",
       "..                     ...     ...               ...\n",
       "80         sad unhappy 9.1     NaN               NaN\n",
       "81       angry furious 8.8     NaN               NaN\n",
       "82       calm peaceful 8.5     NaN               NaN\n",
       "83     strong powerful 9.0     NaN               NaN\n",
       "84        weak fragile 8.4     NaN               NaN\n",
       "\n",
       "[85 rows x 3 columns]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['Word 1', 'Word 2', 'Similarity Index']\n",
    "\n",
    "df = pd.read_csv('custom_semantic_similarity.txt', sep='\\t', header=None, names=columns)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    word_1 = row['Word 1']\n",
    "    word_2 = row['Word 2']\n",
    "\n",
    "    try:\n",
    "        embed_1_neg_samp    = get_embed_neg_sample(word_1)\n",
    "        embed_2_neg_samp    = get_embed_neg_sample(word_2)\n",
    "        embed_1_skip_gram   = get_embed_skip_gram(word_1)\n",
    "        embed_2_skip_gram   = get_embed_skip_gram(word_2)\n",
    "        embed_1_glove       = get_embed_glove(word_1)\n",
    "        embed_2_glove       = get_embed_glove(word_2)\n",
    "\n",
    "    except KeyError:\n",
    "        # Replacing missing embeddings with the embedding of '<UNK>'\n",
    "        embed_1_neg_samp    = get_embed_neg_sample('<UNK>')\n",
    "        embed_2_neg_samp    = get_embed_neg_sample('<UNK>')\n",
    "        embed_1_skip_gram   = get_embed_skip_gram('<UNK>')\n",
    "        embed_2_skip_gram   = get_embed_skip_gram('<UNK>')\n",
    "        embed_1_glove       = get_embed_glove('<UNK>')\n",
    "        embed_2_glove       = get_embed_glove('<UNK>')\n",
    "\n",
    "    # Computing dot product\n",
    "    df.at[index, 'dot_product_neg_samp'] = np.dot(embed_1_neg_samp, embed_2_neg_samp)\n",
    "    df.at[index, 'dot_product_skip_gram'] = np.dot(embed_1_skip_gram, embed_2_skip_gram)\n",
    "    df.at[index, 'dot_product_glove'] = np.dot(embed_1_glove, embed_2_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Similarity Index</th>\n",
       "      <th>dot_product_neg_samp</th>\n",
       "      <th>dot_product_skip_gram</th>\n",
       "      <th>dot_product_glove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>teacher instructor 9.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23783</td>\n",
       "      <td>0.010527</td>\n",
       "      <td>0.094765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doctor physician 9.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23783</td>\n",
       "      <td>0.010527</td>\n",
       "      <td>0.094765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lawyer attorney 9.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23783</td>\n",
       "      <td>0.010527</td>\n",
       "      <td>0.094765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>student pupil 8.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23783</td>\n",
       "      <td>0.010527</td>\n",
       "      <td>0.094765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>city metropolis 8.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23783</td>\n",
       "      <td>0.010527</td>\n",
       "      <td>0.094765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>car automobile 9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23783</td>\n",
       "      <td>0.010527</td>\n",
       "      <td>0.094765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>plane aircraft 9.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23783</td>\n",
       "      <td>0.010527</td>\n",
       "      <td>0.094765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>river stream 7.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23783</td>\n",
       "      <td>0.010527</td>\n",
       "      <td>0.094765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mountain hill 7.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23783</td>\n",
       "      <td>0.010527</td>\n",
       "      <td>0.094765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>computer laptop 8.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23783</td>\n",
       "      <td>0.010527</td>\n",
       "      <td>0.094765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Word 1  Word 2  Similarity Index  dot_product_neg_samp  \\\n",
       "0  teacher instructor 9.3     NaN               NaN               0.23783   \n",
       "1    doctor physician 9.4     NaN               NaN               0.23783   \n",
       "2     lawyer attorney 9.1     NaN               NaN               0.23783   \n",
       "3       student pupil 8.8     NaN               NaN               0.23783   \n",
       "4     city metropolis 8.7     NaN               NaN               0.23783   \n",
       "5      car automobile 9.0     NaN               NaN               0.23783   \n",
       "6      plane aircraft 9.2     NaN               NaN               0.23783   \n",
       "7        river stream 7.9     NaN               NaN               0.23783   \n",
       "8       mountain hill 7.2     NaN               NaN               0.23783   \n",
       "9     computer laptop 8.3     NaN               NaN               0.23783   \n",
       "\n",
       "   dot_product_skip_gram  dot_product_glove  \n",
       "0               0.010527           0.094765  \n",
       "1               0.010527           0.094765  \n",
       "2               0.010527           0.094765  \n",
       "3               0.010527           0.094765  \n",
       "4               0.010527           0.094765  \n",
       "5               0.010527           0.094765  \n",
       "6               0.010527           0.094765  \n",
       "7               0.010527           0.094765  \n",
       "8               0.010527           0.094765  \n",
       "9               0.010527           0.094765  "
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qh/04jfxrcd49g3sj2d522qbjtw0000gn/T/ipykernel_19999/2929932877.py:4: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  correlation_pos, _ = spearmanr(df['Similarity Index'], df['dot_product_skip_gram'])\n",
      "/var/folders/qh/04jfxrcd49g3sj2d522qbjtw0000gn/T/ipykernel_19999/2929932877.py:5: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  correlation_neg, _ = spearmanr(df['Similarity Index'], df['dot_product_neg_samp'])\n",
      "/var/folders/qh/04jfxrcd49g3sj2d522qbjtw0000gn/T/ipykernel_19999/2929932877.py:6: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  correlation_glove, _ = spearmanr(df['Similarity Index'], df['dot_product_glove'])\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Computing the Spearman correlation\n",
    "correlation_pos, _ = spearmanr(df['Similarity Index'], df['dot_product_skip_gram'])\n",
    "correlation_neg, _ = spearmanr(df['Similarity Index'], df['dot_product_neg_samp'])\n",
    "correlation_glove, _ = spearmanr(df['Similarity Index'], df['dot_product_glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation Coefficient of Skipgram: nan\n",
      "Spearman Correlation Coefficient of Skipgram with Negative Sampling: nan\n",
      "Spearman Correlation Coefficient of Glove: nan\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spearman Correlation Coefficient of Skipgram: {correlation_pos:.4f}\")\n",
    "print(f\"Spearman Correlation Coefficient of Skipgram with Negative Sampling: {correlation_neg:.4f}\")\n",
    "print(f\"Spearman Correlation Coefficient of Glove: {correlation_glove:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: nan\n"
     ]
    }
   ],
   "source": [
    "# Finding y_true based on the mean of similarity index in the df\n",
    "y_true = df['Similarity Index'].mean()\n",
    "\n",
    "print(f\"y_true: {y_true:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation Correlation coefficient of Glove (genism): 0.50\n"
     ]
    }
   ],
   "source": [
    "correlation_coefficient = model_genism.evaluate_word_pairs('custom_semantic_similarity.txt')\n",
    "print(f\"Spearman Correlation Correlation coefficient of Glove (genism): {correlation_coefficient[1][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Skipgram | NEG | GloVe | GloVe (genism) | Y_true |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| MSE    | 0.011     | 0.0385     | -0.03     | 0.5     | 5.29   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_for_corpus(model, words):\n",
    "    embeddings = {}\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            index = word2index[word]\n",
    "        except KeyError:\n",
    "            index = word2index['<UNK>']\n",
    "\n",
    "        word_tensor = torch.LongTensor([index])\n",
    "        word_tensor = word_tensor.to(device)\n",
    "\n",
    "        embed_c = model.embedding_center(word_tensor)\n",
    "        embed_o = model.embedding_outside(word_tensor)\n",
    "        embed = (embed_c + embed_o) / 2\n",
    "\n",
    "        # return as dictionary with key as the word and value as the array of its embedding\n",
    "        embeddings[word] = np.array([embed[0][0].item(), embed[0][1].item()])\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_whole_glove = get_embed_for_corpus(model_glove, vocab)\n",
    "embed_whole_neg_skg = get_embed_for_corpus(model_neg, vocab)\n",
    "embed_whole_skg = get_embed_for_corpus(model_glove, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('app/model_gensim.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model_genism, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "with open('app/embed_skipgram_negative.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(embed_whole_neg_skg, pickle_file)\n",
    "\n",
    "print(f\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "with open('app/embed_skipgram.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(embed_whole_skg, pickle_file)\n",
    "\n",
    "print(f\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "with open('app/embed_glove.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(embed_whole_glove, pickle_file)\n",
    "\n",
    "print(f\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, for window size of 2 - skipgram had the highest (7.60) training loss while Negative skipgram had the lowest (1.89). Glove also performed much better compared to skipgram with loss of 2.33.\n",
    "\n",
    "In case of Training time each model were trained for 5000 epoch. Skipgram took the longest time with 310s and Glove took the least amount of time with 34s. Negative sampling took 171s. \n",
    "\n",
    "All three models coded from scratch performed bad compared to Genism. This is because of the small corpus size and window size. All 3 models (Skipgram, Skipgram with negative sampling, Glove) had syantactic and semantic accuracy of 0%. This was expected because of the limitations of our corpus. Glove (Genism) on the other achieved Syntactic accuracy of 55.45% \n",
    "and Semantic Accuracy of 93.87%.\n",
    "\n",
    "Furthermore, for Spearman Correlation Coefficient - Genism outperforms other models with correlation score of `0.5`. The other 3 models showed poor correlation which suggests that predicted rankings do not closely match with ground truth. So our embeddings has poor correlation with human judgement.\n",
    "\n",
    "In conclusion, given the small corpus size, window size and embedding dimension - our 'made from scratch' models performed poorly. Given better hyperparameter tunings like embeddig dimensions, learning rate and even number of epochs, the models can be refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
