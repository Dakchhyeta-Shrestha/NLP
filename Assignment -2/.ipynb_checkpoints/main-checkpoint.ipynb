{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e376a830-5959-4010-8952-1fcbbc29b528",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks and Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a2ad90c-61b6-4615-b640-221ac710bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce77d8fb-2dbd-4cbb-a9bd-3e0503aee9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10a83f97-394e-40a6-bb6c-f6854d0fbfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed for reproducability\n",
    "SEED = 122\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb779dc-cf1f-4948-9ae1-3aae19c57cb3",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eed6e5-e832-424f-b99c-f429ba14619e",
   "metadata": {},
   "source": [
    "Source Credit: https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9ee61e3-9749-41de-a1d4-2b3b1ac0e9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Records: 1\n",
      "Preview:\n",
      "                                                text\n",
      "0  THE BOY WHO LIVED Mr and Mrs Dursley of number...\n"
     ]
    }
   ],
   "source": [
    "#Cleaning the dataset\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_PATH = \"./data/book.txt\"        \n",
    "OUTPUT_TXT = \"./data/cleaned_book.txt\"\n",
    "OUTPUT_CSV = \"./data/cleaned_book.csv\"\n",
    "\n",
    "SMART_MAP = {\n",
    "    \"\\u2018\": \"'\", \"\\u2019\": \"'\", \"\\u201C\": '\"', \"\\u201D\": '\"',\n",
    "    \"\\u2013\": \"-\", \"\\u2014\": \"-\", \"\\u2212\": \"-\",\n",
    "    \"\\u2026\": \"...\",\n",
    "    \"\\u00A0\": \" \",\n",
    "}\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "\n",
    "    # Normalize unicode\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "    # Replace smart punctuation\n",
    "    for k, v in SMART_MAP.items():\n",
    "        s = s.replace(k, v)\n",
    "\n",
    "    # Remove zero-width/invisible characters\n",
    "    s = re.sub(r\"[\\u200B-\\u200D\\uFEFF]\", \"\", s)\n",
    "\n",
    "    # Convert newlines/tabs to spaces (we'll re-control splitting ourselves)\n",
    "    s = re.sub(r\"[\\r\\n\\t]+\", \" \", s)\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s).strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "# ---------- Option A: Treat EACH LINE as one record ----------\n",
    "def load_as_lines(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        lines = [ln.strip(\"\\n\") for ln in f]\n",
    "    # drop empty lines\n",
    "    lines = [ln for ln in lines if ln.strip()]\n",
    "    return lines\n",
    "\n",
    "# ---------- Option B: Treat PARAGRAPHS (blank-line separated) as one record ----------\n",
    "def load_as_paragraphs(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        raw = f.read()\n",
    "    # split on 1+ blank lines\n",
    "    paras = re.split(r\"\\n\\s*\\n+\", raw.strip())\n",
    "    paras = [p for p in paras if p.strip()]\n",
    "    return paras\n",
    "\n",
    "# Choose one:\n",
    "records = load_as_paragraphs(INPUT_PATH)  # or: load_as_lines(INPUT_PATH)\n",
    "\n",
    "# Clean\n",
    "cleaned = [clean_text(r) for r in records]\n",
    "cleaned = [c for c in cleaned if c]  # remove empties\n",
    "\n",
    "# Remove duplicates\n",
    "seen = set()\n",
    "unique_cleaned = []\n",
    "for c in cleaned:\n",
    "    if c not in seen:\n",
    "        seen.add(c)\n",
    "        unique_cleaned.append(c)\n",
    "\n",
    "# Save cleaned TXT (one record per line)\n",
    "# Save cleaned TXT (ALL records concatenated into ONE line)\n",
    "with open(OUTPUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(unique_cleaned))\n",
    "\n",
    "\n",
    "# Also save CSV\n",
    "df = pd.DataFrame({\"text\": unique_cleaned})\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Records:\", len(unique_cleaned))\n",
    "print(\"Preview:\")\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bc0ed90-76b5-40fb-b487-650f5d7db2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 67785\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "alice_dataset = \"./data/cleaned_book.txt\"\n",
    "\n",
    "# Read the data from the file\n",
    "with open(alice_dataset, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Creating list of dictionaries\n",
    "data = data.split(\" .\")\n",
    "data = [{\"text\": row} for row in data]\n",
    "\n",
    "# Creating dataset object\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab52a300-1180-446d-804d-a4c5dbc94ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 54228\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 6779\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 6778\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_test = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# 10% test set and 10% validation set\n",
    "train_test_valid = train_test['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': train_test_valid['test'],\n",
    "    'validation': train_test_valid['train']})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea1f85a2-b48f-4e3d-b6b3-a31e1a2f6c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my poor dear boy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIf you change the index you might notice that sometimes there is no paragraph shown\\nrather an empty string therefore, will have to care of that later.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['train'][333]['text']) # empty string\n",
    "\n",
    "'''\n",
    "If you change the index you might notice that sometimes there is no paragraph shown\n",
    "rather an empty string therefore, will have to care of that later.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37bf18-ceb6-47d6-ade6-10bf97426c22",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a5dad-80df-4bd4-8d9f-68825dc7fa0e",
   "metadata": {},
   "source": [
    "Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0231498-aeeb-48c5-b315-3d24019296d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53795734-c42d-43cf-9dc3-08911c80b377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17502 Sso sweet Dudders\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i = random.randrange(len(dataset[\"train\"]))\n",
    "print(i, dataset[\"train\"][i][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f945a75d-cdf3-4297-99f3-972c8bd6d8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d838ed931164277a0c9ada402d077af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a904e737dd4f4e3c9f0a796d3d7c095f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64529d5446124f519f38391981d866fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6778 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dyou', 'really', 'think', 'theres', 'a', 'chamber', 'of', 'secrets', '?', 'ron', 'asked', 'hermione']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "#function to tokenize\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}  \n",
    "\n",
    "#map the function to each example\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})\n",
    "print(tokenized_dataset['train'][33]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0ec85-a4d5-4d17-a2f8-09679e808565",
   "metadata": {},
   "source": [
    "# Numericializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5d5dacb-522f-4f91-88c5-bcddcc24540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8415dfd-2467-4810-8743-7b387beff8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11049\n",
      "['<unk>', '<eos>', 'the', 'and', 'to', 'of', 'a', 'he', 'harry', 'was']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "counter = Counter(tok for sent in tokenized_dataset[\"train\"][\"tokens\"] for tok in sent)\n",
    "vocab = Vocab(counter, min_freq=3, specials=[\"<unk>\", \"<eos>\"])\n",
    "\n",
    "print(len(vocab))\n",
    "print(vocab.itos[:10])   # torchtext 0.6.0 uses .itos instead of get_itos()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68ebb960-5c01-414b-8c89-8ac27ed3aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/vocab_lm.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c2a0c-d9f6-4cad-b8c5-383af39e7696",
   "metadata": {},
   "source": [
    "After loading and splitting the personally chosen dataset a DatasetDictonary is created. Then on that object the preprocessing steps are applied. Firstly, we tokenize the dataset using torchtext's get_tokenizer. The tokenize_data function is applied to each example where the text column is removed and a new tokens column containing the tokenized text is added.\n",
    "\n",
    "Then our vocabulary is made using the build_vocab_from_iterator method from torchtext. We use the training dataset, and consider words that has occured at least three times. This is done to make sure that our vocab does not get too big. Then we add <unk> to signify unknown and <eos> to signify end of sentence. After all this the vocab size came out to be 928."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170197f-0e5f-415a-9b3c-ffeb6376ada5",
   "metadata": {},
   "source": [
    "# Prepare the batch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "297265af-39a4-4670-add6-e4ee15654de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['tokens']:         \n",
    "            #appends eos so we know it ends....so model learn how to end...                             \n",
    "            tokens = example['tokens'].append('<eos>')   \n",
    "            #numericalize          \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size #get the int number of batches...\n",
    "    data = data[:num_batches * batch_size] #make the batch evenly, and cut out any remaining                      \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data #[batch size, bunch of tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8acc8f61-6bff-45ec-845b-7308beffa26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "PAD_IDX = 0\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def lm_collate(batch):\n",
    "    batch = [b for b in batch if len(b) >= 2]\n",
    "    x = [b[:-1] for b in batch]\n",
    "    y = [b[1:]  for b in batch]\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=PAD_IDX)\n",
    "    y = pad_sequence(y, batch_first=True, padding_value=PAD_IDX)\n",
    "    return x, y\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=lm_collate)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lm_collate)\n",
    "test_loader  = DataLoader(test_data,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=lm_collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391fec5c-e989-4110-bc19-5a4fdfc7d975",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d9000e3-3865-43e9-8fc9-7251778e77b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "                \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim = hid_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, \n",
    "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        #embedding: [batch size, seq len, emb_dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)      \n",
    "        #output: [batch size, seq len, hid_dim]\n",
    "        #hidden = h, c = [num_layers * direction, seq len, hid_dim)\n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        #prediction: [batch size, seq_len, vocab size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5642f7-4413-4be7-ba09-6c706e09d13d",
   "metadata": {},
   "source": [
    "An LSTM architecture incorporates a cell state and a set of gating mechanisms that regulate information flow across time steps, enabling it to model long-range dependencies. The input gate determines how much new information is written to the cell state, while the forget gate controls the extent to which information from the previous cell state is retained or discarded. The cell state functions as a persistent memory that is updated through these gates. The output gate then governs how much of the cell state is exposed as the hidden state, which directly influences the modelâ€™s output at each step.\n",
    "In our implementation, the pipeline first maps discrete tokens into dense embedding representations. These embeddings are subsequently processed by stacked LSTM layers to capture temporal structure and sequential dependencies. To improve generalisation, dropout is applied to the embeddings during training, reducing overfitting by randomly masking a proportion of input features. Finally, a linear projection layer transforms the LSTM outputs into vocabulary-level logits, enabling next-word prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74deb1-96e3-4230-ae8a-5427de7b086d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf65cdc3-27c8-42ad-8207-630945ba1683",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                \n",
    "hid_dim = 1024               \n",
    "num_layers = 2                \n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "156e25f9-3330-4e2e-a2ad-96534f841747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 39,433,001 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4055d44-d62f-400f-92c5-21132aa13b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e92ce05c-c48a-4388-8b1c-020ddc19bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, bunch of tokens]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69997664-1bef-44aa-9519-1448900c2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a2ab96e-4e42-4b2d-adea-3f0dc08f1868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 312.451\n",
      "\tValid Perplexity: 220.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 225.549\n",
      "\tValid Perplexity: 176.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 187.804\n",
      "\tValid Perplexity: 154.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 164.768\n",
      "\tValid Perplexity: 141.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 148.604\n",
      "\tValid Perplexity: 131.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 136.536\n",
      "\tValid Perplexity: 124.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 126.996\n",
      "\tValid Perplexity: 119.081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 118.952\n",
      "\tValid Perplexity: 114.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 112.176\n",
      "\tValid Perplexity: 111.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 106.361\n",
      "\tValid Perplexity: 109.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 101.225\n",
      "\tValid Perplexity: 107.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 96.841\n",
      "\tValid Perplexity: 105.192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 92.677\n",
      "\tValid Perplexity: 103.966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 89.043\n",
      "\tValid Perplexity: 102.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 85.887\n",
      "\tValid Perplexity: 102.211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 82.717\n",
      "\tValid Perplexity: 101.347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 79.968\n",
      "\tValid Perplexity: 100.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 77.475\n",
      "\tValid Perplexity: 100.839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 73.095\n",
      "\tValid Perplexity: 100.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 71.156\n",
      "\tValid Perplexity: 99.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 69.635\n",
      "\tValid Perplexity: 99.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 67.373\n",
      "\tValid Perplexity: 99.320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 66.399\n",
      "\tValid Perplexity: 99.192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 65.612\n",
      "\tValid Perplexity: 99.240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 64.536\n",
      "\tValid Perplexity: 99.296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.750\n",
      "\tValid Perplexity: 99.243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.524\n",
      "\tValid Perplexity: 99.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.230\n",
      "\tValid Perplexity: 99.199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.253\n",
      "\tValid Perplexity: 99.170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.219\n",
      "\tValid Perplexity: 99.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.124\n",
      "\tValid Perplexity: 99.138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.127\n",
      "\tValid Perplexity: 99.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.051\n",
      "\tValid Perplexity: 99.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.109\n",
      "\tValid Perplexity: 99.138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.103\n",
      "\tValid Perplexity: 99.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.008\n",
      "\tValid Perplexity: 99.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.181\n",
      "\tValid Perplexity: 99.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.093\n",
      "\tValid Perplexity: 99.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.105\n",
      "\tValid Perplexity: 99.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.152\n",
      "\tValid Perplexity: 99.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.138\n",
      "\tValid Perplexity: 99.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.083\n",
      "\tValid Perplexity: 99.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.073\n",
      "\tValid Perplexity: 99.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.132\n",
      "\tValid Perplexity: 99.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.134\n",
      "\tValid Perplexity: 99.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.086\n",
      "\tValid Perplexity: 99.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.096\n",
      "\tValid Perplexity: 99.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.096\n",
      "\tValid Perplexity: 99.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.108\n",
      "\tValid Perplexity: 99.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 63.146\n",
      "\tValid Perplexity: 99.136\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "seq_len  = 50 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model/best-val-lstm_lm.pt')\n",
    "\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1334de-58b1-4481-a7ce-cb1aecbfe52d",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4e1f6e8-9171-441b-87fb-9b021cd0e426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_data: torch.Size([8, 14499]) test_data: torch.Size([8, 14782]) seq_len: 25\n",
      "Test Perplexity: 99.920\n"
     ]
    }
   ],
   "source": [
    "eval_batch_size = 8\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, eval_batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'], vocab, eval_batch_size)\n",
    "\n",
    "seq_len = min(25, valid_data.shape[1]-1, test_data.shape[1]-1)\n",
    "print(\"valid_data:\", valid_data.shape, \"test_data:\", test_data.shape, \"seq_len:\", seq_len)\n",
    "\n",
    "test_loss = evaluate(model, test_data, criterion, eval_batch_size, seq_len, device)\n",
    "print(f\"Test Perplexity: {math.exp(test_loss):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "684c2e28-becf-430f-8847-f12e9df9cfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp shape: torch.Size([128, 905]) tgt shape: torch.Size([128, 905])\n",
      "total target tokens: 115840\n",
      "unique target ids (first 50): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "min id: 0 max id: 11048\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(valid_loader))\n",
    "inp, tgt = batch  # (B, T), (B, T)\n",
    "\n",
    "print(\"inp shape:\", inp.shape, \"tgt shape:\", tgt.shape)\n",
    "\n",
    "# Since you are NOT padding, everything is a real token.\n",
    "print(\"total target tokens:\", tgt.numel())\n",
    "print(\"unique target ids (first 50):\", torch.unique(tgt)[:50].tolist())\n",
    "print(\"min id:\", int(tgt.min()), \"max id:\", int(tgt.max()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf172a24-a1d2-4fe8-83eb-503e212b8575",
   "metadata": {},
   "source": [
    "# Real World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9925f4a3-a1f9-4f4e-91b8-5ea7ea0efa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.itos\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be02143c-e5e9-4af8-847d-5fe73ba345a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "harry potter is a bit of a coincidence\n",
      "\n",
      "0.7\n",
      "harry potter is being a lot of time to find in\n",
      "\n",
      "0.75\n",
      "harry potter is being a lot of time to find in\n",
      "\n",
      "0.8\n",
      "harry potter is being a lot of time to find in\n",
      "\n",
      "1.0\n",
      "harry potter is being dressed an ounce of time making this cup\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Harry potter is '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
