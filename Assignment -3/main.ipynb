{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, math, time, json, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0706c",
   "metadata": {},
   "source": [
    "# ETL: Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb70a5d",
   "metadata": {},
   "source": [
    "For this assignment, the language I chose is Nepali. And the dataset is taken from HuggingFace: https://huggingface.co/datasets/Helsinki-NLP/opus-100/viewer/en-ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c0bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "SRC_LANG = \"en\"\n",
    "TARG_LANG = \"ne\"\n",
    "\n",
    "# Change this if your dataset name/config differs in your environment\n",
    "# This assumes you already used this dataset earlier in your notebook.\n",
    "# If you used a local dataset object named `dataset`, this will be overwritten.\n",
    "dataset = load_dataset(\"Helsinki-NLP/opus-100\", f\"{SRC_LANG}-{TARG_LANG}\")\n",
    "\n",
    "print(dataset)\n",
    "print(\"sizes:\", {k: len(dataset[k]) for k in dataset.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ae465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset sizes (safe slicing) \n",
    "N_TRAIN = 20000\n",
    "N_VALID = 2000\n",
    "N_TEST  = 2000\n",
    "\n",
    "train_ds = dataset[\"train\"].select(range(min(N_TRAIN, len(dataset[\"train\"]))))\n",
    "valid_ds = dataset[\"validation\"].select(range(min(N_VALID, len(dataset[\"validation\"]))))\n",
    "test_ds  = dataset[\"test\"].select(range(min(N_TEST, len(dataset[\"test\"]))))\n",
    "\n",
    "print(\"train/valid/test sizes:\", len(train_ds), len(valid_ds), len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc5ab3",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82818c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nepalitokenizers import WordPiece\n",
    "\n",
    "spacy_en = spacy.blank(\"en\")  # fast tokenizer\n",
    "wp_ne = WordPiece()           # Nepali WordPiece tokenizer\n",
    "\n",
    "def tok_en(text: str):\n",
    "    return [t.text for t in spacy_en(text.lower().strip())]\n",
    "\n",
    "def tok_ne(text: str):\n",
    "    enc = wp_ne.encode(text.strip())\n",
    "    return enc.tokens\n",
    "\n",
    "# Quick check\n",
    "print(tok_en(\"The clipboard could not be signed.\")[:10])\n",
    "print(tok_ne(\"क्लिपबोर्ड साइन गर्न सकिएन ।\")[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138bb767",
   "metadata": {},
   "source": [
    "# Text numericalisation and batch collation with padding for DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "special_symbols = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "def build_vocab_from_tokenizer(ds, tokenizer_fn):\n",
    "    counter = Counter()\n",
    "    for ex in ds:\n",
    "        counter.update(tokenizer_fn(ex))\n",
    "    v = Vocab(counter, specials=special_symbols)\n",
    "    v.unk_index = v.stoi[\"<unk>\"]\n",
    "    return v\n",
    "\n",
    "# Build vocab from TRAIN ONLY\n",
    "vocab_transform = {}\n",
    "vocab_transform[SRC_LANG]  = build_vocab_from_tokenizer(train_ds[SRC_LANG], tok_en)\n",
    "vocab_transform[TARG_LANG] = build_vocab_from_tokenizer(train_ds[TARG_LANG], tok_ne)\n",
    "\n",
    "UNK_IDX = vocab_transform[TARG_LANG].stoi[\"<unk>\"]\n",
    "PAD_IDX = vocab_transform[TARG_LANG].stoi[\"<pad>\"]\n",
    "BOS_IDX = vocab_transform[TARG_LANG].stoi[\"<bos>\"]\n",
    "EOS_IDX = vocab_transform[TARG_LANG].stoi[\"<eos>\"]\n",
    "\n",
    "SRC_PAD_IDX = vocab_transform[SRC_LANG].stoi[\"<pad>\"]\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "print(\"vocab sizes:\", len(vocab_transform[SRC_LANG]), len(vocab_transform[TARG_LANG]))\n",
    "print(\"UNK, PAD, BOS, EOS:\", UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614af024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_LEN = 96\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def numericalize(vocab, tokens):\n",
    "    return [vocab.stoi.get(t, vocab.stoi[\"<unk>\"]) for t in tokens]\n",
    "\n",
    "def text_to_ids(text, lang):\n",
    "    if lang == SRC_LANG:\n",
    "        toks = tok_en(text)\n",
    "        vocab = vocab_transform[SRC_LANG]\n",
    "        pad = SRC_PAD_IDX\n",
    "    else:\n",
    "        toks = tok_ne(text)\n",
    "        vocab = vocab_transform[TARG_LANG]\n",
    "        pad = TRG_PAD_IDX\n",
    "\n",
    "    ids = numericalize(vocab, toks)\n",
    "    ids = ids[: MAX_LEN - 2]  \n",
    "    if lang == SRC_LANG:\n",
    "        return [vocab_transform[SRC_LANG].stoi[\"<bos>\"]] + ids + [vocab_transform[SRC_LANG].stoi[\"<eos>\"]]\n",
    "    else:\n",
    "        return [BOS_IDX] + ids + [EOS_IDX]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for item in batch:\n",
    "        src_ids = text_to_ids(item[SRC_LANG], SRC_LANG)\n",
    "        trg_ids = text_to_ids(item[TARG_LANG], TARG_LANG)\n",
    "        src_batch.append(torch.tensor(src_ids, dtype=torch.long))\n",
    "        trg_batch.append(torch.tensor(trg_ids, dtype=torch.long))\n",
    "        src_len_batch.append(len(src_ids))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=SRC_PAD_IDX, batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=TRG_PAD_IDX, batch_first=True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "train_loader_length = len(train_loader)\n",
    "val_loader_length   = len(valid_loader)\n",
    "test_loader_length  = len(test_loader)\n",
    "\n",
    "# Sanity UNK rates\n",
    "src, _, trg = next(iter(train_loader))\n",
    "print(\"SRC UNK %:\", (src == vocab_transform[SRC_LANG].stoi[\"<unk>\"]).float().mean().item())\n",
    "print(\"TRG UNK %:\", (trg == UNK_IDX).float().mean().item())\n",
    "print(\"batch shapes:\", src.shape, trg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, atten_type, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.atten_type = atten_type\n",
    "        self.device = device\n",
    "\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "       # general\n",
    "        self.W = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\n",
    "        # additive\n",
    "        self.Wq = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.Wk = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.v  = nn.Linear(self.head_dim, 1, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float, device=device))\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        if self.atten_type == \"general\":\n",
    "            energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        elif self.atten_type == \"additive\":\n",
    "            # memory-safe additive: compute scores without expanding head_dim dimension\n",
    "            # e_ij = v^T tanh(Wq q_i + Wk k_j)\n",
    "            # We still need [B,H,Lq,Lk, D] for tanh, but MAX_LEN=96 and BATCH=4 keeps it safe.\n",
    "            Qe = self.Wq(Q).unsqueeze(3)   # [B,H,Lq,1,D]\n",
    "            Ke = self.Wk(K).unsqueeze(2)   # [B,H,1,Lk,D]\n",
    "            e  = torch.tanh(Qe + Ke)       # [B,H,Lq,Lk,D]\n",
    "            energy = self.v(e).squeeze(-1) # [B,H,Lq,Lk]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown atten_type: {self.atten_type}\")\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        x = torch.matmul(self.dropout(attention), V)  # [B,H,Lq,D]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba747f60",
   "metadata": {},
   "source": [
    "# Encoder and Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c618a05",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90459598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, atten_type, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, atten_type, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        _src = self.feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        return src\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, atten_type, device, max_length=MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(hid_dim, n_heads, pf_dim, dropout, atten_type, device)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        batch_size, src_len = src.shape\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, atten_type, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm   = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm         = nn.LayerNorm(hid_dim)\n",
    "\n",
    "        self.self_attention        = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, atten_type, device)\n",
    "        self.encoder_attention     = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, atten_type, device)\n",
    "        self.feedforward           = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout               = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        _trg = self.feedforward(trg)\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        return trg, attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, atten_type, device, max_length=MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(hid_dim, n_heads, pf_dim, dropout, atten_type, device)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        batch_size, trg_len = trg.shape\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "        return output, attention\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        return (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "        trg_sub_mask = trg_sub_mask.unsqueeze(0).unsqueeze(1)\n",
    "        return trg_pad_mask & trg_sub_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c7f8a7",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227943d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, src_len, trg in loader:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, _ = model(src, trg[:, :-1])  # teacher forcing\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg_out = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg_out)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / loader_length\n",
    "\n",
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, src_len, trg in loader:\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:, :-1])\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg_out = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, trg_out)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75411ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim  = len(vocab_transform[SRC_LANG])\n",
    "output_dim = len(vocab_transform[TARG_LANG])\n",
    "\n",
    "# Smaller model for stability\n",
    "HID_DIM = 128\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 256\n",
    "DEC_PF_DIM = 256\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "LR = 8e-4  \n",
    "EPOCHS = 5\n",
    "clip = 1.0\n",
    "\n",
    "ATTEN_TYPES = [\"general\", \"multiplicative\", \"additive\"]\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, \"weight\") and m.weight is not None and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "models = {}\n",
    "optimizers = {}\n",
    "histories = {k: {\"train\": [], \"valid\": []} for k in ATTEN_TYPES}\n",
    "best_valid = {k: float(\"inf\") for k in ATTEN_TYPES}\n",
    "\n",
    "for atten_type in ATTEN_TYPES:\n",
    "    print(\"\\n===== BUILD:\", atten_type, \"=====\")\n",
    "\n",
    "    enc = Encoder(input_dim, HID_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, atten_type, device, max_length=MAX_LEN)\n",
    "    dec = Decoder(output_dim, HID_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, atten_type, device, max_length=MAX_LEN)\n",
    "    model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "    model.apply(initialize_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    models[atten_type] = model\n",
    "    optimizers[atten_type] = optimizer\n",
    "\n",
    "    print(\"model ready\")\n",
    "\n",
    "for atten_type in ATTEN_TYPES:\n",
    "    model = models[atten_type]\n",
    "    optimizer = optimizers[atten_type]\n",
    "\n",
    "    print(\"\\n===== TRAIN:\", atten_type, \"=====\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        t0 = time.time()\n",
    "        tr_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "        va_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "        t1 = time.time()\n",
    "\n",
    "        histories[atten_type][\"train\"].append(tr_loss)\n",
    "        histories[atten_type][\"valid\"].append(va_loss)\n",
    "\n",
    "        if va_loss < best_valid[atten_type]:\n",
    "            best_valid[atten_type] = va_loss\n",
    "            torch.save(\n",
    "                {\"atten_type\": atten_type,\n",
    "                 \"state_dict\": model.state_dict(),\n",
    "                 \"input_dim\": input_dim,\n",
    "                 \"output_dim\": output_dim},\n",
    "                f\"model_{atten_type}.pt\"\n",
    "            )\n",
    "\n",
    "        print(f\"{atten_type} | epoch {epoch+1:02d} | train {tr_loss:.3f} ppl {math.exp(tr_loss):.2f} | valid {va_loss:.3f} ppl {math.exp(va_loss):.2f} | {t1-t0:.1f}s\")\n",
    "\n",
    "    # free MPS cache between trainings (keep model in dict for later evaluation/plots)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"saved checkpoints:\", [f\"model_{k}.pt\" for k in ATTEN_TYPES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72959d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required table\n",
    "print(\"Attention | Train Loss | Train PPL | Valid Loss | Valid PPL\")\n",
    "rows = []\n",
    "for k in ATTEN_TYPES:\n",
    "    tr = histories[k][\"train\"][-1]\n",
    "    va = histories[k][\"valid\"][-1]\n",
    "    rows.append((k, tr, math.exp(tr), va, math.exp(va)))\n",
    "    print(f\"{k:>12} | {tr:9.3f} | {math.exp(tr):9.2f} | {va:9.3f} | {math.exp(va):9.2f}\")\n",
    "\n",
    "# Graphs \n",
    "for k in ATTEN_TYPES:\n",
    "    plt.figure()\n",
    "    plt.plot(histories[k][\"train\"], label=\"train\")\n",
    "    plt.plot(histories[k][\"valid\"], label=\"valid\")\n",
    "    plt.title(f\"Loss curves: {k}\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1f38e",
   "metadata": {},
   "source": [
    "| Attentions | Training Loss | Traning PPL | Validation Loss | Validation PPL |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| General Attention    | 2.141     | 8.51     | 3.158     | 23.53     |\n",
    "| Additive Attention    | 2.112     | 8.27     | 3.149    | 23.32    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909382e5",
   "metadata": {},
   "source": [
    "### Explanation of the Results Table (Loss and Perplexity)\n",
    "\n",
    "This table compares the performance of the **General Attention** and **Additive Attention** models using **training loss**, **training perplexity (PPL)**, **validation loss**, and **validation perplexity**. Both models show very similar behaviour, but **Additive Attention performs slightly better overall**.\n",
    "\n",
    "On the training set, **Additive Attention** achieves a lower loss (**2.112**) and lower PPL (**8.27**) compared to **General Attention** (loss **2.141**, PPL **8.51**). This suggests that the Additive Attention model fits the training data marginally better and learns stronger token level patterns during optimisation.\n",
    "\n",
    "On the validation set, Additive Attention again produces slightly lower loss (**3.149**) and lower PPL (**23.32**) than General Attention (loss **3.158**, PPL **23.53**). Since lower validation loss and PPL indicate better prediction on unseen data, this shows that Additive Attention generalises a little better.\n",
    "\n",
    "A noticeable difference between training and validation values exists for both models, indicating mild overfitting. However, the consistent improvement across both metrics suggests that **Additive Attention provides a more flexible alignment mechanism** for source to target mapping in this English to Nepali translation task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbfd39",
   "metadata": {},
   "source": [
    "# Evaluation and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65771216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_ids_to_text(ids):\n",
    "    itos = vocab_transform[TARG_LANG].itos\n",
    "    out_tokens = []\n",
    "    for i in ids:\n",
    "        i = int(i)\n",
    "        if i < 0 or i >= len(itos):\n",
    "            continue\n",
    "        tok = itos[i]\n",
    "        if tok in {\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"}:\n",
    "            continue\n",
    "        out_tokens.append(tok)\n",
    "    out = \" \".join(out_tokens).replace(\" ##\", \"\").strip()\n",
    "    if out == \"\":\n",
    "        return \"<no_output>\"\n",
    "    return out\n",
    "\n",
    "def translate(model, en_text, max_len=40, min_len=3):\n",
    "    model.eval()\n",
    "\n",
    "    src_ids = text_to_ids(en_text, SRC_LANG)\n",
    "    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    src_mask = (src_tensor != SRC_PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_ids = [BOS_IDX]\n",
    "\n",
    "    for step in range(max_len):\n",
    "        trg_tensor = torch.tensor(trg_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        L = trg_tensor.size(1)\n",
    "        trg_mask = torch.tril(torch.ones((L, L), device=device)).bool().unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out, attn = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        logits = out[:, -1, :].squeeze(0)\n",
    "\n",
    "        # block UNK always\n",
    "        logits[UNK_IDX] = -1e10\n",
    "\n",
    "        # prevent EOS too early\n",
    "        if len(trg_ids) < min_len:\n",
    "            logits[EOS_IDX] = -1e10\n",
    "\n",
    "        next_id = int(torch.argmax(logits).item())\n",
    "        trg_ids.append(next_id)\n",
    "\n",
    "        if next_id == EOS_IDX:\n",
    "            break\n",
    "\n",
    "    return decode_ids_to_text(trg_ids), attn, src_ids, trg_ids\n",
    "\n",
    "# Show a few samples\n",
    "samples = [train_ds[i][SRC_LANG] for i in [0, 1, 2, 3, 4]]\n",
    "for s in samples:\n",
    "    print(\"EN:\", s)\n",
    "    for k in ATTEN_TYPES:\n",
    "        ne, _, _, _ = translate(models[k], s)\n",
    "        print(k, \"NE:\", ne)\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f81b4",
   "metadata": {},
   "source": [
    "### Attention Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_heatmap(attn, src_ids, trg_ids, title):\n",
    "    if attn is None:\n",
    "        print(\"no attention\")\n",
    "        return\n",
    "    # [1, heads, trg_len, src_len] -> average heads\n",
    "    A = attn.squeeze(0).mean(0).detach().cpu().numpy()\n",
    "\n",
    "    src_tokens = [vocab_transform[SRC_LANG].itos[i] for i in src_ids]\n",
    "    trg_tokens = [vocab_transform[TARG_LANG].itos[i] for i in trg_ids]\n",
    "\n",
    "    # limit for readability\n",
    "    max_src = min(len(src_tokens), 20)\n",
    "    max_trg = min(len(trg_tokens), 20)\n",
    "    A = A[:max_trg, :max_src]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(A, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(max_src), src_tokens[:max_src], rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(max_trg), trg_tokens[:max_trg])\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "example = train_ds[0][SRC_LANG]\n",
    "print(\"Example EN:\", example)\n",
    "\n",
    "for k in ATTEN_TYPES:\n",
    "    ne, attn, src_ids, trg_ids = translate(models[k], example)\n",
    "    print(k, \"NE:\", ne)\n",
    "    attention_heatmap(attn, src_ids, trg_ids, f\"Attention map: {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcdc10f",
   "metadata": {},
   "source": [
    "### Analysis of Results\n",
    "\n",
    "Based on the discussion in the previous sections, **Additive Attention** produced the best overall performance among the three attention mechanisms. It achieved the **lowest loss and perplexity** during evaluation, indicating more accurate next token prediction compared to the General and Multiplicative variants. Therefore, the Additive Attention model was selected for deployment in the web application.\n",
    "\n",
    "Despite this relative improvement, the translation quality across all three models was still weak. This may be due to **insufficient training epochs**, which can limit learning, or **overfitting**, where the model performs well on the training data but does not generalise effectively to unseen inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efccdcd",
   "metadata": {},
   "source": [
    "### User Interface and Model Integration\n",
    "\n",
    "For this assignment, the user interface was developed using **Dash**. The entire interface, along with the required model integration, is implemented within the `app.py` file. The UI is intentionally kept simple and consists of a text input field for the user query, a **Translate** button, basic input validation, and an output section to display the translation result. Screenshots demonstrating the interface and its functionality are included in the `README.md` file inside the A3 folder.\n",
    "\n",
    "The trained model is integrated into the interface through a clear and structured pipeline. First, the saved vocabulary is loaded, and the trained model is initialised using its stored parameters. Among the three attention mechanisms implemented, the **Additive Attention** model was selected for deployment as it showed the best overall performance during evaluation. Once the user provides an input sentence, the text is tokenised and numericalised using the loaded vocabulary, after which tensors are created and passed to the model. The model then predicts the output by selecting the token with the highest probability at each decoding step, and the final result is displayed to the user.\n",
    "\n",
    "### User Interaction Flow\n",
    "\n",
    "The interaction flow of the application is as follows:\n",
    "\n",
    "- The user enters an English sentence into the input field  \n",
    "- The user clicks the **Translate** button  \n",
    "- The translated Nepali output is displayed on the screen  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
