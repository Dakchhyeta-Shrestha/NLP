{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchtext\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from datasets import load_dataset\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import random, math, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_general.pt exists? True\n",
      "model_multiplicative.pt exists? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for f in [\"model_general.pt\", \"model_multiplicative.pt\"]:\n",
    "    print(f, \"exists?\", os.path.exists(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00df026e97448d1b5b55c4fe0e561a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map: 100%|##########| 2000/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d676a4efaa04e2ba516ec272074a945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map: 100%|##########| 406381/406381 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9d1555a43c451d9795966f3a91f010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map: 100%|##########| 2000/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FINAL VERIFICATION ---\n",
      "EN data: _Inv\n",
      "NE data: Inv\n",
      "SUCCESS: Data is repaired.\n"
     ]
    }
   ],
   "source": [
    "# 1. Force a total reset by reloading the dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"opus100\", \"en-ne\")\n",
    "\n",
    "# 2. Use a completely new function name to avoid cache matching\n",
    "def repair_dataset_columns(batch):\n",
    "    # Explicitly pull the strings from the nested dictionary\n",
    "    return {\n",
    "        'en': batch['translation']['en'],\n",
    "        'ne': batch['translation']['ne']\n",
    "    }\n",
    "\n",
    "# 3. Map with CACHE DISABLED\n",
    "dataset = dataset.map(\n",
    "    repair_dataset_columns, \n",
    "    remove_columns=['translation'],\n",
    "    load_from_cache_file=False  # <--- THIS IS THE KEY\n",
    ")\n",
    "\n",
    "# 4. VERIFY - If this is still \"en\", there is a deeper issue\n",
    "print(\"--- FINAL VERIFICATION ---\")\n",
    "example = dataset['train'][0]\n",
    "print(f\"EN data: {example['en']}\")\n",
    "print(f\"NE data: {example['ne']}\")\n",
    "\n",
    "if example['en'] == \"en\" or len(example['en']) < 3:\n",
    "    print(\"CRITICAL ERROR: Data is still corrupted. Restart your Notebook Kernel.\")\n",
    "else:\n",
    "    print(\"SUCCESS: Data is repaired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/valid/test sizes: 50000 2000 2000\n"
     ]
    }
   ],
   "source": [
    "# ===== Dataset sizes (safe slicing) =====\n",
    "N_TRAIN = 50000\n",
    "N_VALID = 5000\n",
    "N_TEST  = 5000\n",
    "\n",
    "train_ds = dataset[\"train\"].select(range(min(N_TRAIN, len(dataset[\"train\"]))))\n",
    "valid_ds = dataset[\"validation\"].select(range(min(N_VALID, len(dataset[\"validation\"]))))\n",
    "test_ds  = dataset[\"test\"].select(range(min(N_TEST, len(dataset[\"test\"]))))\n",
    "\n",
    "print(\"train/valid/test sizes:\", len(train_ds), len(valid_ds), len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "SRC_LANG= 'en'\n",
    "TARG_LANG = 'ne'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from nepalitokenizers import WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[\"en\"] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[\"ne\"] = WordPiece()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_token(batch, lang):\n",
    "    text = batch[lang]\n",
    "    tok = token_transform[lang]\n",
    "\n",
    "    if lang == \"en\":\n",
    "        # spacy tokenizer returns tokens (strings) or token objects\n",
    "        raw = tok(text.lower().strip())\n",
    "        tokens = [getattr(t, \"text\", t) for t in raw]\n",
    "    else:\n",
    "        enc = tok.encode(text.strip())\n",
    "        tokens = enc.tokens if hasattr(enc, \"tokens\") else enc\n",
    "\n",
    "    # remove obvious control tokens if present\n",
    "    remove = {\"[CLS]\", \"[SEP]\", \"en\", \"ne\", \"<s>\", \"</s>\"}\n",
    "    tokens = [t for t in tokens if t not in remove]\n",
    "\n",
    "    return {lang: tokens}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737dd3466e3547a589081113f8c17b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2a2518ec214f1fbc440186a41647fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdca6d90ac14538b568b2b4b8212681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b0123331fb41f58a940a2b66ecf57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee231e718e29480b8a11b99af487528c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2b4283833a4806ad4e9ffe30d300a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample EN tokens: ['_', 'inv']\n",
      "sample NE tokens: ['inv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize the limited datasets\n",
    "tokenized_train = train_ds.map(get_data_token, fn_kwargs={\"lang\": SRC_LANG})\n",
    "tokenized_train = tokenized_train.map(get_data_token, fn_kwargs={\"lang\": TARG_LANG})\n",
    "\n",
    "tokenized_valid = valid_ds.map(get_data_token, fn_kwargs={\"lang\": SRC_LANG})\n",
    "tokenized_valid = tokenized_valid.map(get_data_token, fn_kwargs={\"lang\": TARG_LANG})\n",
    "\n",
    "tokenized_test  = test_ds.map(get_data_token, fn_kwargs={\"lang\": SRC_LANG})\n",
    "tokenized_test  = tokenized_test.map(get_data_token, fn_kwargs={\"lang\": TARG_LANG})\n",
    "\n",
    "# quick peek\n",
    "print(\"sample EN tokens:\", tokenized_train[0][SRC_LANG][:10])\n",
    "print(\"sample NE tokens:\", tokenized_train[0][TARG_LANG][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab sizes: 15889 9209\n",
      "UNK, PAD, BOS, EOS: 0 1 2 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchtext.vocab import Vocab\n",
    "from collections import Counter\n",
    "\n",
    "special_symbols = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "vocab_transform = {}\n",
    "\n",
    "for ln in [SRC_LANG, TARG_LANG]:\n",
    "    counter = Counter()\n",
    "    for tokens in tokenized_train[ln]:\n",
    "        counter.update(tokens)\n",
    "    v = Vocab(counter, specials=special_symbols)\n",
    "    v.unk_index = v.stoi[\"<unk>\"]\n",
    "    vocab_transform[ln] = v\n",
    "\n",
    "UNK_IDX = vocab_transform[TARG_LANG].stoi[\"<unk>\"]\n",
    "PAD_IDX = vocab_transform[TARG_LANG].stoi[\"<pad>\"]\n",
    "BOS_IDX = vocab_transform[TARG_LANG].stoi[\"<bos>\"]\n",
    "EOS_IDX = vocab_transform[TARG_LANG].stoi[\"<eos>\"]\n",
    "\n",
    "SRC_PAD_IDX = vocab_transform[SRC_LANG].stoi[\"<pad>\"]\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "print(\"vocab sizes:\", len(vocab_transform[SRC_LANG]), len(vocab_transform[TARG_LANG]))\n",
    "print(\"UNK, PAD, BOS, EOS:\", UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC UNK %: 0.0\n",
      "TRG UNK %: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def numericalize(vocab, tokens):\n",
    "    return [vocab.stoi.get(t, vocab.stoi[\"<unk>\"]) for t in tokens]\n",
    "\n",
    "def text_to_ids(text, lang):\n",
    "    # uses the already-defined tokenizers for consistency\n",
    "    if lang == \"en\":\n",
    "        raw = token_transform[\"en\"](text.lower().strip())\n",
    "        toks = [getattr(t, \"text\", t) for t in raw]\n",
    "    else:\n",
    "        enc = token_transform[\"ne\"].encode(text.strip())\n",
    "        toks = enc.tokens if hasattr(enc, \"tokens\") else enc\n",
    "\n",
    "    remove = {\"[CLS]\", \"[SEP]\", \"en\", \"ne\", \"<s>\", \"</s>\"}\n",
    "    toks = [t for t in toks if t not in remove]\n",
    "\n",
    "    ids = numericalize(vocab_transform[lang], toks)\n",
    "    return [BOS_IDX] + ids + [EOS_IDX]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for item in batch:\n",
    "        src_text = item[SRC_LANG]\n",
    "        trg_text = item[TARG_LANG]\n",
    "\n",
    "        src_ids = text_to_ids(src_text, SRC_LANG)\n",
    "        trg_ids = text_to_ids(trg_text, TARG_LANG)\n",
    "\n",
    "        src_batch.append(torch.tensor(src_ids, dtype=torch.long))\n",
    "        trg_batch.append(torch.tensor(trg_ids, dtype=torch.long))\n",
    "        src_len_batch.append(len(src_ids))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=SRC_PAD_IDX, batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=TRG_PAD_IDX, batch_first=True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "train_loader_length = len(train_loader)\n",
    "val_loader_length   = len(valid_loader)\n",
    "test_loader_length  = len(test_loader)\n",
    "\n",
    "# sanity: unk rates\n",
    "src, _, trg = next(iter(train_loader))\n",
    "print(\"SRC UNK %:\", (src == vocab_transform[SRC_LANG].stoi[\"<unk>\"]).float().mean().item())\n",
    "print(\"TRG UNK %:\", (trg == UNK_IDX).float().mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTEN_TYPES now: ['additive']\n"
     ]
    }
   ],
   "source": [
    "ATTEN_TYPES = [\"additive\"]\n",
    "print(\"ATTEN_TYPES now:\", ATTEN_TYPES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, atten_type, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.atten_type = atten_type\n",
    "        self.device = device\n",
    "\n",
    "        # projections\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        # multiplicative attention parameter\n",
    "        self.W = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\n",
    "        # additive attention parameters\n",
    "        self.Wq = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.Wk = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.v  = nn.Linear(self.head_dim, 1, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float, device=device))\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        # [B, H, L, D]\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        if self.atten_type == \"general\":\n",
    "            energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        elif self.atten_type == \"multiplicative\":\n",
    "            Qw = self.W(Q)  # [B, H, Lq, D]\n",
    "            energy = torch.matmul(Qw, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        elif self.atten_type == \"additive\":\n",
    "            # expand for pairwise (Lq x Lk)\n",
    "            Qe = self.Wq(Q).unsqueeze(3)  # [B,H,Lq,1,D]\n",
    "            Ke = self.Wk(K).unsqueeze(2)  # [B,H,1,Lk,D]\n",
    "            e = torch.tanh(Qe + Ke)       # [B,H,Lq,Lk,D]\n",
    "            energy = self.v(e).squeeze(-1)  # [B,H,Lq,Lk]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown atten_type: {self.atten_type}\")\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        x = torch.matmul(self.dropout(attention), V)  # [B,H,Lq,D]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()         # [B,Lq,H,D]\n",
    "        x = x.view(batch_size, -1, self.hid_dim)       # [B,Lq,hid]\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, hid_dim]\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, atten_type, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, atten_type, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "\n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, atten_type, device, max_length = 512):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.atten_type = atten_type\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, atten_type,device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, atten_type, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, atten_type, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, atten_type, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, atten_type, device,max_length = 512):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, atten_type, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.params = {'encoder': encoder, 'decoder': decoder,\n",
    "                       'src_pad_idx': src_pad_idx, 'trg_pad_idx': trg_pad_idx}\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models ready: ['additive']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "input_dim  = len(vocab_transform[SRC_LANG])\n",
    "output_dim = len(vocab_transform[TARG_LANG])\n",
    "\n",
    "HID_DIM = 128\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, \"weight\") and m.weight is not None and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "ATTEN_TYPES = [\"additive\"]\n",
    "\n",
    "models = {}\n",
    "optimizers = {}\n",
    "histories = {}  # {atten_type: {\"train\":[], \"valid\":[]}}\n",
    "\n",
    "for atten_type in ATTEN_TYPES:\n",
    "    enc = Encoder(input_dim, HID_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, atten_type, device)\n",
    "    dec = Decoder(output_dim, HID_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, atten_type, device)\n",
    "\n",
    "    model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "    model.apply(initialize_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "    models[atten_type] = model\n",
    "    optimizers[atten_type] = optimizer\n",
    "    histories[atten_type] = {\"train\": [], \"valid\": []}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
    "clip = 1.0\n",
    "\n",
    "print(\"Models ready:\", list(models.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models: ['additive']\n"
     ]
    }
   ],
   "source": [
    "print(\"models:\", list(models.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # SHIFTING LOGIC: Feed the target without the last token (<eos>)\n",
    "        output, _ = model(src, trg[:, :-1])\n",
    "        \n",
    "        # RESHAPE: Compare output against target shifted by one (removes <bos>)\n",
    "        # This forces the model to predict the NEXT word in the sequence\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "\n",
    "        trg_output = trg[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(output, trg_output)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # Prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, src_len, trg in loader:\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            # Shifting logic: Feed target without the last token\n",
    "            output, _ = model(src, trg[:, :-1])\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg_output = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, trg_output)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training: general =====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m     13\u001b[39m     t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     train_loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n\u001b[32m     16\u001b[39m     t1 = time.time()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, loader, optimizer, criterion, clip, loader_length)\u001b[39m\n\u001b[32m     19\u001b[39m trg_output = trg[:, \u001b[32m1\u001b[39m:].contiguous().view(-\u001b[32m1\u001b[39m)\n\u001b[32m     20\u001b[39m loss = criterion(output, trg_output)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Prevent exploding gradients\u001b[39;00m\n\u001b[32m     25\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import time, math\n",
    "\n",
    "EPOCHS = 6  # keep fast for 1000 samples\n",
    "\n",
    "best_valid = {k: float(\"inf\") for k in models.keys()}\n",
    "\n",
    "for atten_type in ATTEN_TYPES:\n",
    "    print(\"\\n===== Training:\", atten_type, \"=====\")\n",
    "    model = models[atten_type]\n",
    "    optimizer = optimizers[atten_type]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        t0 = time.time()\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "        t1 = time.time()\n",
    "\n",
    "        histories[atten_type][\"train\"].append(train_loss)\n",
    "        histories[atten_type][\"valid\"].append(valid_loss)\n",
    "\n",
    "        if valid_loss < best_valid[atten_type]:\n",
    "            best_valid[atten_type] = valid_loss\n",
    "            torch.save({\"atten_type\": atten_type,\n",
    "                        \"state_dict\": model.state_dict(),\n",
    "                        \"input_dim\": input_dim,\n",
    "                        \"output_dim\": output_dim}, f\"model_{atten_type}.pt\")\n",
    "\n",
    "        print(f\"{atten_type} | epoch {epoch+1:02d} | train {train_loss:.3f} ppl {math.exp(train_loss):.2f} | valid {valid_loss:.3f} ppl {math.exp(valid_loss):.2f} | {t1-t0:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attentions | Train Loss | Train PPL | Valid Loss | Valid PPL\n",
      "      general |     1.819 |      6.17 |     3.389 |     29.64\n",
      "multiplicative |     1.988 |      7.30 |     3.410 |     30.27\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAttentions | Train Loss | Train PPL | Valid Loss | Valid PPL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m atten_type \u001b[38;5;129;01min\u001b[39;00m ATTEN_TYPES:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     tr = \u001b[43mhistories\u001b[49m\u001b[43m[\u001b[49m\u001b[43matten_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     85\u001b[39m     va = histories[atten_type][\u001b[33m\"\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m]\n\u001b[32m     86\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00matten_type\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>13\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m9.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath.exp(tr)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m9.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m9.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath.exp(va)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m9.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def translate_with_attention(model, en_text, max_len=30):\n",
    "    model.eval()\n",
    "\n",
    "    # source ids + tokens\n",
    "    src_ids = text_to_ids(en_text, SRC_LANG)\n",
    "    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    src_mask = (src_tensor != SRC_PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    src_tokens = [vocab_transform[SRC_LANG].itos[i] for i in src_ids]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_ids = [BOS_IDX]\n",
    "    attentions = None\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.tensor(trg_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        L = trg_tensor.size(1)\n",
    "        trg_mask = torch.tril(torch.ones((L, L), device=device)).bool().unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out, attn = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        probs = out[:, -1, :].squeeze(0)\n",
    "        probs[UNK_IDX] = -1e10\n",
    "        next_id = int(probs.argmax().item())\n",
    "        trg_ids.append(next_id)\n",
    "\n",
    "        attentions = attn  # last layer attention [1, heads, trg_len, src_len]\n",
    "\n",
    "        if next_id == EOS_IDX:\n",
    "            break\n",
    "\n",
    "    trg_tokens = [vocab_transform[TARG_LANG].itos[i] for i in trg_ids]\n",
    "    # clean tokens\n",
    "    skip = {\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"}\n",
    "    out_tokens = [t for t in trg_tokens if t not in skip]\n",
    "    out_text = \" \".join(out_tokens).replace(\" ##\", \"\").strip()\n",
    "\n",
    "    return out_text, attentions, src_tokens, trg_tokens\n",
    "\n",
    "def plot_loss_curves(histories):\n",
    "    for atten_type, h in histories.items():\n",
    "        plt.figure()\n",
    "        plt.plot(h[\"train\"], label=\"train\")\n",
    "        plt.plot(h[\"valid\"], label=\"valid\")\n",
    "        plt.title(f\"Loss curves: {atten_type}\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def attention_heatmap(attn, src_tokens, trg_tokens, title=\"Attention\"):\n",
    "    # attn: [1, heads, trg_len, src_len]\n",
    "    if attn is None:\n",
    "        print(\"No attention returned\")\n",
    "        return\n",
    "    attn = attn.squeeze(0).mean(0).cpu().numpy()  # [trg_len, src_len]\n",
    "\n",
    "    # limit sizes for readability\n",
    "    max_src = min(len(src_tokens), 20)\n",
    "    max_trg = min(len(trg_tokens), 20)\n",
    "\n",
    "    attn = attn[:max_trg, :max_src]\n",
    "    src = src_tokens[:max_src]\n",
    "    trg = trg_tokens[:max_trg]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(attn, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(src)), src, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(trg)), trg)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===== Performance table =====\n",
    "print(\"Attentions | Train Loss | Train PPL | Valid Loss | Valid PPL\")\n",
    "for atten_type in ATTEN_TYPES:\n",
    "    tr = histories[atten_type][\"train\"][-1]\n",
    "    va = histories[atten_type][\"valid\"][-1]\n",
    "    print(f\"{atten_type:>13} | {tr:9.3f} | {math.exp(tr):9.2f} | {va:9.3f} | {math.exp(va):9.2f}\")\n",
    "\n",
    "# ===== Loss plots =====\n",
    "plot_loss_curves(histories)\n",
    "\n",
    "# ===== Attention maps (one example per model) =====\n",
    "example = train_ds[0][\"en\"]\n",
    "print(\"\\nExample EN:\", example)\n",
    "\n",
    "for atten_type in ATTEN_TYPES:\n",
    "    model = models[atten_type]\n",
    "    out, attn, src_toks, trg_toks = translate_with_attention(model, example, max_len=30)\n",
    "    print(\"\\n\", atten_type, \"NE:\", out)\n",
    "    attention_heatmap(attn, src_toks, trg_toks, title=f\"Attention map ({atten_type})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
